/home/tf2387/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Using device: cuda
Setting up data...
Warning: Custom test directory not found. Creating dummy test loader.
Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']
Creating medium model...
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 64, 32, 32]           1,728
       BatchNorm2d-2           [-1, 64, 32, 32]             128
              ReLU-3           [-1, 64, 32, 32]               0
            Conv2d-4           [-1, 64, 32, 32]          36,864
       BatchNorm2d-5           [-1, 64, 32, 32]             128
              ReLU-6           [-1, 64, 32, 32]               0
         Dropout2d-7           [-1, 64, 32, 32]               0
            Conv2d-8           [-1, 64, 32, 32]          36,864
       BatchNorm2d-9           [-1, 64, 32, 32]             128
             ReLU-10           [-1, 64, 32, 32]               0
       BasicBlock-11           [-1, 64, 32, 32]               0
           Conv2d-12           [-1, 64, 32, 32]          36,864
      BatchNorm2d-13           [-1, 64, 32, 32]             128
             ReLU-14           [-1, 64, 32, 32]               0
        Dropout2d-15           [-1, 64, 32, 32]               0
           Conv2d-16           [-1, 64, 32, 32]          36,864
      BatchNorm2d-17           [-1, 64, 32, 32]             128
             ReLU-18           [-1, 64, 32, 32]               0
       BasicBlock-19           [-1, 64, 32, 32]               0
           Conv2d-20           [-1, 64, 32, 32]          36,864
      BatchNorm2d-21           [-1, 64, 32, 32]             128
             ReLU-22           [-1, 64, 32, 32]               0
        Dropout2d-23           [-1, 64, 32, 32]               0
           Conv2d-24           [-1, 64, 32, 32]          36,864
      BatchNorm2d-25           [-1, 64, 32, 32]             128
             ReLU-26           [-1, 64, 32, 32]               0
       BasicBlock-27           [-1, 64, 32, 32]               0
           Conv2d-28          [-1, 128, 16, 16]          73,728
      BatchNorm2d-29          [-1, 128, 16, 16]             256
             ReLU-30          [-1, 128, 16, 16]               0
        Dropout2d-31          [-1, 128, 16, 16]               0
           Conv2d-32          [-1, 128, 16, 16]         147,456
      BatchNorm2d-33          [-1, 128, 16, 16]             256
           Conv2d-34          [-1, 128, 16, 16]           8,192
      BatchNorm2d-35          [-1, 128, 16, 16]             256
             ReLU-36          [-1, 128, 16, 16]               0
       BasicBlock-37          [-1, 128, 16, 16]               0
           Conv2d-38          [-1, 128, 16, 16]         147,456
      BatchNorm2d-39          [-1, 128, 16, 16]             256
             ReLU-40          [-1, 128, 16, 16]               0
        Dropout2d-41          [-1, 128, 16, 16]               0
           Conv2d-42          [-1, 128, 16, 16]         147,456
      BatchNorm2d-43          [-1, 128, 16, 16]             256
             ReLU-44          [-1, 128, 16, 16]               0
       BasicBlock-45          [-1, 128, 16, 16]               0
           Conv2d-46          [-1, 128, 16, 16]         147,456
      BatchNorm2d-47          [-1, 128, 16, 16]             256
             ReLU-48          [-1, 128, 16, 16]               0
        Dropout2d-49          [-1, 128, 16, 16]               0
           Conv2d-50          [-1, 128, 16, 16]         147,456
      BatchNorm2d-51          [-1, 128, 16, 16]             256
             ReLU-52          [-1, 128, 16, 16]               0
       BasicBlock-53          [-1, 128, 16, 16]               0
           Conv2d-54          [-1, 128, 16, 16]         147,456
      BatchNorm2d-55          [-1, 128, 16, 16]             256
             ReLU-56          [-1, 128, 16, 16]               0
        Dropout2d-57          [-1, 128, 16, 16]               0
           Conv2d-58          [-1, 128, 16, 16]         147,456
      BatchNorm2d-59          [-1, 128, 16, 16]             256
             ReLU-60          [-1, 128, 16, 16]               0
       BasicBlock-61          [-1, 128, 16, 16]               0
           Conv2d-62            [-1, 256, 8, 8]         294,912
      BatchNorm2d-63            [-1, 256, 8, 8]             512
             ReLU-64            [-1, 256, 8, 8]               0
        Dropout2d-65            [-1, 256, 8, 8]               0
           Conv2d-66            [-1, 256, 8, 8]         589,824
      BatchNorm2d-67            [-1, 256, 8, 8]             512
           Conv2d-68            [-1, 256, 8, 8]          32,768
      BatchNorm2d-69            [-1, 256, 8, 8]             512
             ReLU-70            [-1, 256, 8, 8]               0
       BasicBlock-71            [-1, 256, 8, 8]               0
           Conv2d-72            [-1, 256, 8, 8]         589,824
      BatchNorm2d-73            [-1, 256, 8, 8]             512
             ReLU-74            [-1, 256, 8, 8]               0
        Dropout2d-75            [-1, 256, 8, 8]               0
           Conv2d-76            [-1, 256, 8, 8]         589,824
      BatchNorm2d-77            [-1, 256, 8, 8]             512
             ReLU-78            [-1, 256, 8, 8]               0
       BasicBlock-79            [-1, 256, 8, 8]               0
           Conv2d-80            [-1, 256, 8, 8]         589,824
      BatchNorm2d-81            [-1, 256, 8, 8]             512
             ReLU-82            [-1, 256, 8, 8]               0
        Dropout2d-83            [-1, 256, 8, 8]               0
           Conv2d-84            [-1, 256, 8, 8]         589,824
      BatchNorm2d-85            [-1, 256, 8, 8]             512
             ReLU-86            [-1, 256, 8, 8]               0
       BasicBlock-87            [-1, 256, 8, 8]               0
AdaptiveAvgPool2d-88            [-1, 256, 1, 1]               0
          Dropout-89                  [-1, 256]               0
           Linear-90                   [-1, 10]           2,570
================================================================
Total params: 4,623,178
Trainable params: 4,623,178
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 25.25
Params size (MB): 17.64
Estimated Total Size (MB): 42.90
----------------------------------------------------------------
None
Starting training for 200 epochs...

Epoch 1/200
Train Batch: 50/352 | Loss: 2.720 | Acc: 10.97% (702/6400)
Train Batch: 100/352 | Loss: 2.464 | Acc: 14.41% (1844/12800)
Train Batch: 150/352 | Loss: 2.337 | Acc: 16.40% (3149/19200)
Train Batch: 200/352 | Loss: 2.264 | Acc: 17.73% (4540/25600)
Train Batch: 250/352 | Loss: 2.209 | Acc: 18.95% (6063/32000)
Train Batch: 300/352 | Loss: 2.165 | Acc: 19.76% (7588/38400)
Train Batch: 350/352 | Loss: 2.128 | Acc: 20.73% (9287/44800)
Train Batch: 352/352 | Loss: 2.127 | Acc: 20.74% (9335/45000)
Validation | Loss: 1.809 | Acc: 31.62% (1581/5000)
New best model with accuracy: 0.3162

Epoch 2/200
Train Batch: 50/352 | Loss: 1.877 | Acc: 27.56% (1764/6400)
Train Batch: 100/352 | Loss: 1.851 | Acc: 28.59% (3659/12800)
Train Batch: 150/352 | Loss: 1.834 | Acc: 29.43% (5651/19200)
Train Batch: 200/352 | Loss: 1.817 | Acc: 30.51% (7811/25600)
Train Batch: 250/352 | Loss: 1.800 | Acc: 31.20% (9983/32000)
Train Batch: 300/352 | Loss: 1.780 | Acc: 32.11% (12330/38400)
Train Batch: 350/352 | Loss: 1.765 | Acc: 32.87% (14724/44800)
Train Batch: 352/352 | Loss: 1.764 | Acc: 32.92% (14813/45000)
Validation | Loss: 1.553 | Acc: 42.48% (2124/5000)
New best model with accuracy: 0.4248

Epoch 3/200
Train Batch: 50/352 | Loss: 1.647 | Acc: 38.02% (2433/6400)
Train Batch: 100/352 | Loss: 1.630 | Acc: 39.01% (4993/12800)
Train Batch: 150/352 | Loss: 1.615 | Acc: 39.54% (7591/19200)
Train Batch: 200/352 | Loss: 1.592 | Acc: 40.35% (10329/25600)
Train Batch: 250/352 | Loss: 1.572 | Acc: 41.19% (13180/32000)
Train Batch: 300/352 | Loss: 1.551 | Acc: 42.20% (16203/38400)
Train Batch: 350/352 | Loss: 1.534 | Acc: 42.92% (19228/44800)
Train Batch: 352/352 | Loss: 1.534 | Acc: 42.93% (19320/45000)
Validation | Loss: 1.313 | Acc: 49.86% (2493/5000)
New best model with accuracy: 0.4986

Epoch 4/200
Train Batch: 50/352 | Loss: 1.381 | Acc: 49.30% (3155/6400)
Train Batch: 100/352 | Loss: 1.369 | Acc: 50.22% (6428/12800)
Train Batch: 150/352 | Loss: 1.344 | Acc: 51.44% (9876/19200)
Train Batch: 200/352 | Loss: 1.333 | Acc: 51.93% (13294/25600)
Train Batch: 250/352 | Loss: 1.315 | Acc: 52.64% (16846/32000)
Train Batch: 300/352 | Loss: 1.300 | Acc: 53.13% (20403/38400)
Train Batch: 350/352 | Loss: 1.290 | Acc: 53.47% (23956/44800)
Train Batch: 352/352 | Loss: 1.290 | Acc: 53.48% (24064/45000)
Validation | Loss: 1.044 | Acc: 61.52% (3076/5000)
New best model with accuracy: 0.6152

Epoch 5/200
Train Batch: 50/352 | Loss: 1.184 | Acc: 57.05% (3651/6400)
Train Batch: 100/352 | Loss: 1.179 | Acc: 57.79% (7397/12800)
Train Batch: 150/352 | Loss: 1.165 | Acc: 58.28% (11190/19200)
Train Batch: 200/352 | Loss: 1.154 | Acc: 58.73% (15036/25600)
Train Batch: 250/352 | Loss: 1.145 | Acc: 59.15% (18929/32000)
Train Batch: 300/352 | Loss: 1.139 | Acc: 59.58% (22880/38400)
Train Batch: 350/352 | Loss: 1.126 | Acc: 60.00% (26878/44800)
Train Batch: 352/352 | Loss: 1.127 | Acc: 59.98% (26993/45000)
Validation | Loss: 1.069 | Acc: 64.18% (3209/5000)
New best model with accuracy: 0.6418

Epoch 6/200
Train Batch: 50/352 | Loss: 1.039 | Acc: 62.39% (3993/6400)
Train Batch: 100/352 | Loss: 1.032 | Acc: 62.84% (8043/12800)
Train Batch: 150/352 | Loss: 1.028 | Acc: 63.17% (12129/19200)
Train Batch: 200/352 | Loss: 1.019 | Acc: 63.66% (16297/25600)
Train Batch: 250/352 | Loss: 1.015 | Acc: 63.94% (20461/32000)
Train Batch: 300/352 | Loss: 1.003 | Acc: 64.50% (24768/38400)
Train Batch: 350/352 | Loss: 0.996 | Acc: 64.84% (29049/44800)
Train Batch: 352/352 | Loss: 0.995 | Acc: 64.88% (29194/45000)
Validation | Loss: 0.854 | Acc: 69.56% (3478/5000)
New best model with accuracy: 0.6956

Epoch 7/200
Train Batch: 50/352 | Loss: 0.927 | Acc: 67.12% (4296/6400)
Train Batch: 100/352 | Loss: 0.916 | Acc: 67.64% (8658/12800)
Train Batch: 150/352 | Loss: 0.911 | Acc: 67.94% (13044/19200)
Train Batch: 200/352 | Loss: 0.901 | Acc: 68.29% (17482/25600)
Train Batch: 250/352 | Loss: 0.896 | Acc: 68.48% (21914/32000)
Train Batch: 300/352 | Loss: 0.892 | Acc: 68.66% (26364/38400)
Train Batch: 350/352 | Loss: 0.889 | Acc: 68.78% (30815/44800)
Train Batch: 352/352 | Loss: 0.889 | Acc: 68.77% (30948/45000)
Validation | Loss: 0.686 | Acc: 75.20% (3760/5000)
New best model with accuracy: 0.7520

Epoch 8/200
Train Batch: 50/352 | Loss: 0.817 | Acc: 71.95% (4605/6400)
Train Batch: 100/352 | Loss: 0.825 | Acc: 71.61% (9166/12800)
Train Batch: 150/352 | Loss: 0.831 | Acc: 71.36% (13702/19200)
Train Batch: 200/352 | Loss: 0.821 | Acc: 71.84% (18392/25600)
Train Batch: 250/352 | Loss: 0.819 | Acc: 71.90% (23009/32000)
Train Batch: 300/352 | Loss: 0.821 | Acc: 71.76% (27555/38400)
Train Batch: 350/352 | Loss: 0.817 | Acc: 71.89% (32207/44800)
Train Batch: 352/352 | Loss: 0.818 | Acc: 71.88% (32344/45000)
Validation | Loss: 0.737 | Acc: 74.38% (3719/5000)

Epoch 9/200
Train Batch: 50/352 | Loss: 0.791 | Acc: 72.34% (4630/6400)
Train Batch: 100/352 | Loss: 0.790 | Acc: 72.69% (9304/12800)
Train Batch: 150/352 | Loss: 0.780 | Acc: 73.08% (14032/19200)
Train Batch: 200/352 | Loss: 0.781 | Acc: 73.17% (18731/25600)
Train Batch: 250/352 | Loss: 0.780 | Acc: 73.28% (23451/32000)
Train Batch: 300/352 | Loss: 0.776 | Acc: 73.41% (28191/38400)
Train Batch: 350/352 | Loss: 0.773 | Acc: 73.50% (32928/44800)
Train Batch: 352/352 | Loss: 0.773 | Acc: 73.50% (33074/45000)
Validation | Loss: 0.693 | Acc: 75.82% (3791/5000)
New best model with accuracy: 0.7582

Epoch 10/200
Train Batch: 50/352 | Loss: 0.732 | Acc: 75.05% (4803/6400)
Train Batch: 100/352 | Loss: 0.736 | Acc: 74.94% (9592/12800)
Train Batch: 150/352 | Loss: 0.734 | Acc: 74.92% (14384/19200)
Train Batch: 200/352 | Loss: 0.731 | Acc: 75.07% (19217/25600)
Train Batch: 250/352 | Loss: 0.730 | Acc: 75.20% (24065/32000)
Train Batch: 300/352 | Loss: 0.733 | Acc: 75.15% (28856/38400)
Train Batch: 350/352 | Loss: 0.730 | Acc: 75.19% (33686/44800)
Train Batch: 352/352 | Loss: 0.729 | Acc: 75.21% (33843/45000)
Validation | Loss: 0.689 | Acc: 75.76% (3788/5000)

Epoch 11/200
Train Batch: 50/352 | Loss: 0.700 | Acc: 75.38% (4824/6400)
Train Batch: 100/352 | Loss: 0.706 | Acc: 75.59% (9676/12800)
Train Batch: 150/352 | Loss: 0.711 | Acc: 75.60% (14515/19200)
Train Batch: 200/352 | Loss: 0.704 | Acc: 75.78% (19400/25600)
Train Batch: 250/352 | Loss: 0.701 | Acc: 76.03% (24329/32000)
Train Batch: 300/352 | Loss: 0.702 | Acc: 76.07% (29209/38400)
Train Batch: 350/352 | Loss: 0.702 | Acc: 76.05% (34070/44800)
Train Batch: 352/352 | Loss: 0.702 | Acc: 76.03% (34214/45000)
Validation | Loss: 0.672 | Acc: 77.40% (3870/5000)
New best model with accuracy: 0.7740

Epoch 12/200
Train Batch: 50/352 | Loss: 0.664 | Acc: 77.66% (4970/6400)
Train Batch: 100/352 | Loss: 0.664 | Acc: 77.35% (9901/12800)
Train Batch: 150/352 | Loss: 0.672 | Acc: 77.07% (14798/19200)
Train Batch: 200/352 | Loss: 0.680 | Acc: 76.83% (19668/25600)
Train Batch: 250/352 | Loss: 0.679 | Acc: 76.83% (24585/32000)
Train Batch: 300/352 | Loss: 0.680 | Acc: 76.82% (29499/38400)
Train Batch: 350/352 | Loss: 0.681 | Acc: 76.77% (34391/44800)
Train Batch: 352/352 | Loss: 0.680 | Acc: 76.77% (34547/45000)
Validation | Loss: 0.550 | Acc: 80.12% (4006/5000)
New best model with accuracy: 0.8012

Epoch 13/200
Train Batch: 50/352 | Loss: 0.653 | Acc: 77.44% (4956/6400)
Train Batch: 100/352 | Loss: 0.666 | Acc: 77.32% (9897/12800)
Train Batch: 150/352 | Loss: 0.670 | Acc: 77.29% (14839/19200)
Train Batch: 200/352 | Loss: 0.665 | Acc: 77.39% (19813/25600)
Train Batch: 250/352 | Loss: 0.666 | Acc: 77.45% (24783/32000)
Train Batch: 300/352 | Loss: 0.666 | Acc: 77.44% (29736/38400)
Train Batch: 350/352 | Loss: 0.665 | Acc: 77.41% (34681/44800)
Train Batch: 352/352 | Loss: 0.665 | Acc: 77.43% (34842/45000)
Validation | Loss: 0.542 | Acc: 80.48% (4024/5000)
New best model with accuracy: 0.8048

Epoch 14/200
Train Batch: 50/352 | Loss: 0.636 | Acc: 78.78% (5042/6400)
Train Batch: 100/352 | Loss: 0.641 | Acc: 78.60% (10061/12800)
Train Batch: 150/352 | Loss: 0.641 | Acc: 78.53% (15078/19200)
Train Batch: 200/352 | Loss: 0.642 | Acc: 78.49% (20094/25600)
Train Batch: 250/352 | Loss: 0.645 | Acc: 78.33% (25064/32000)
Train Batch: 300/352 | Loss: 0.644 | Acc: 78.41% (30110/38400)
Train Batch: 350/352 | Loss: 0.647 | Acc: 78.30% (35077/44800)
Train Batch: 352/352 | Loss: 0.647 | Acc: 78.26% (35219/45000)
Validation | Loss: 0.619 | Acc: 77.96% (3898/5000)

Epoch 15/200
Train Batch: 50/352 | Loss: 0.596 | Acc: 79.66% (5098/6400)
Train Batch: 100/352 | Loss: 0.621 | Acc: 78.80% (10086/12800)
Train Batch: 150/352 | Loss: 0.622 | Acc: 78.80% (15130/19200)
Train Batch: 200/352 | Loss: 0.626 | Acc: 78.71% (20149/25600)
Train Batch: 250/352 | Loss: 0.629 | Acc: 78.67% (25174/32000)
Train Batch: 300/352 | Loss: 0.629 | Acc: 78.70% (30221/38400)
Train Batch: 350/352 | Loss: 0.632 | Acc: 78.62% (35223/44800)
Train Batch: 352/352 | Loss: 0.632 | Acc: 78.62% (35379/45000)
Validation | Loss: 0.620 | Acc: 79.34% (3967/5000)

Epoch 16/200
Train Batch: 50/352 | Loss: 0.618 | Acc: 78.72% (5038/6400)
Train Batch: 100/352 | Loss: 0.623 | Acc: 79.07% (10121/12800)
Train Batch: 150/352 | Loss: 0.615 | Acc: 79.20% (15206/19200)
Train Batch: 200/352 | Loss: 0.612 | Acc: 79.33% (20308/25600)
Train Batch: 250/352 | Loss: 0.616 | Acc: 79.20% (25345/32000)
Train Batch: 300/352 | Loss: 0.615 | Acc: 79.31% (30455/38400)
Train Batch: 350/352 | Loss: 0.616 | Acc: 79.18% (35472/44800)
Train Batch: 352/352 | Loss: 0.617 | Acc: 79.15% (35617/45000)
Validation | Loss: 0.614 | Acc: 78.42% (3921/5000)

Epoch 17/200
Train Batch: 50/352 | Loss: 0.595 | Acc: 79.64% (5097/6400)
Train Batch: 100/352 | Loss: 0.599 | Acc: 79.57% (10185/12800)
Train Batch: 150/352 | Loss: 0.604 | Acc: 79.54% (15271/19200)
Train Batch: 200/352 | Loss: 0.612 | Acc: 79.10% (20249/25600)
Train Batch: 250/352 | Loss: 0.611 | Acc: 79.23% (25353/32000)
Train Batch: 300/352 | Loss: 0.606 | Acc: 79.35% (30471/38400)
Train Batch: 350/352 | Loss: 0.606 | Acc: 79.41% (35574/44800)
Train Batch: 352/352 | Loss: 0.606 | Acc: 79.41% (35733/45000)
Validation | Loss: 0.571 | Acc: 80.40% (4020/5000)

Epoch 18/200
Train Batch: 50/352 | Loss: 0.616 | Acc: 79.41% (5082/6400)
Train Batch: 100/352 | Loss: 0.608 | Acc: 79.72% (10204/12800)
Train Batch: 150/352 | Loss: 0.605 | Acc: 79.81% (15323/19200)
Train Batch: 200/352 | Loss: 0.600 | Acc: 79.95% (20467/25600)
Train Batch: 250/352 | Loss: 0.601 | Acc: 79.83% (25546/32000)
Train Batch: 300/352 | Loss: 0.602 | Acc: 79.84% (30659/38400)
Train Batch: 350/352 | Loss: 0.600 | Acc: 79.81% (35753/44800)
Train Batch: 352/352 | Loss: 0.599 | Acc: 79.82% (35921/45000)
Validation | Loss: 0.503 | Acc: 82.04% (4102/5000)
New best model with accuracy: 0.8204

Epoch 19/200
Train Batch: 50/352 | Loss: 0.564 | Acc: 81.02% (5185/6400)
Train Batch: 100/352 | Loss: 0.574 | Acc: 80.54% (10309/12800)
Train Batch: 150/352 | Loss: 0.583 | Acc: 80.30% (15417/19200)
Train Batch: 200/352 | Loss: 0.583 | Acc: 80.25% (20545/25600)
Train Batch: 250/352 | Loss: 0.581 | Acc: 80.34% (25710/32000)
Train Batch: 300/352 | Loss: 0.583 | Acc: 80.27% (30822/38400)
Train Batch: 350/352 | Loss: 0.588 | Acc: 80.10% (35887/44800)
Train Batch: 352/352 | Loss: 0.588 | Acc: 80.08% (36037/45000)
Validation | Loss: 0.522 | Acc: 81.96% (4098/5000)

Epoch 20/200
Train Batch: 50/352 | Loss: 0.581 | Acc: 80.33% (5141/6400)
Train Batch: 100/352 | Loss: 0.567 | Acc: 80.88% (10352/12800)
Train Batch: 150/352 | Loss: 0.575 | Acc: 80.66% (15487/19200)
Train Batch: 200/352 | Loss: 0.580 | Acc: 80.56% (20624/25600)
Train Batch: 250/352 | Loss: 0.579 | Acc: 80.67% (25816/32000)
Train Batch: 300/352 | Loss: 0.577 | Acc: 80.72% (30995/38400)
Train Batch: 350/352 | Loss: 0.579 | Acc: 80.64% (36126/44800)
Train Batch: 352/352 | Loss: 0.579 | Acc: 80.65% (36291/45000)
Validation | Loss: 0.550 | Acc: 79.96% (3998/5000)

Epoch 21/200
Train Batch: 50/352 | Loss: 0.570 | Acc: 80.86% (5175/6400)
Train Batch: 100/352 | Loss: 0.569 | Acc: 80.59% (10315/12800)
Train Batch: 150/352 | Loss: 0.569 | Acc: 80.95% (15543/19200)
Train Batch: 200/352 | Loss: 0.573 | Acc: 80.71% (20663/25600)
Train Batch: 250/352 | Loss: 0.569 | Acc: 80.86% (25874/32000)
Train Batch: 300/352 | Loss: 0.569 | Acc: 80.84% (31042/38400)
Train Batch: 350/352 | Loss: 0.569 | Acc: 80.92% (36251/44800)
Train Batch: 352/352 | Loss: 0.569 | Acc: 80.93% (36419/45000)
Validation | Loss: 0.541 | Acc: 80.98% (4049/5000)

Epoch 22/200
Train Batch: 50/352 | Loss: 0.548 | Acc: 81.56% (5220/6400)
Train Batch: 100/352 | Loss: 0.557 | Acc: 81.23% (10398/12800)
Train Batch: 150/352 | Loss: 0.561 | Acc: 81.09% (15570/19200)
Train Batch: 200/352 | Loss: 0.560 | Acc: 81.16% (20777/25600)
Train Batch: 250/352 | Loss: 0.560 | Acc: 81.13% (25962/32000)
Train Batch: 300/352 | Loss: 0.567 | Acc: 80.93% (31077/38400)
Train Batch: 350/352 | Loss: 0.566 | Acc: 80.97% (36276/44800)
Train Batch: 352/352 | Loss: 0.566 | Acc: 80.97% (36437/45000)
Validation | Loss: 0.486 | Acc: 83.26% (4163/5000)
New best model with accuracy: 0.8326

Epoch 23/200
Train Batch: 50/352 | Loss: 0.562 | Acc: 80.98% (5183/6400)
Train Batch: 100/352 | Loss: 0.562 | Acc: 81.11% (10382/12800)
Train Batch: 150/352 | Loss: 0.572 | Acc: 80.79% (15512/19200)
Train Batch: 200/352 | Loss: 0.566 | Acc: 80.94% (20721/25600)
Train Batch: 250/352 | Loss: 0.566 | Acc: 80.88% (25883/32000)
Train Batch: 300/352 | Loss: 0.565 | Acc: 80.91% (31069/38400)
Train Batch: 350/352 | Loss: 0.566 | Acc: 80.91% (36248/44800)
Train Batch: 352/352 | Loss: 0.566 | Acc: 80.91% (36408/45000)
Validation | Loss: 0.534 | Acc: 82.38% (4119/5000)

Epoch 24/200
Train Batch: 50/352 | Loss: 0.544 | Acc: 81.78% (5234/6400)
Train Batch: 100/352 | Loss: 0.542 | Acc: 82.02% (10498/12800)
Train Batch: 150/352 | Loss: 0.549 | Acc: 81.47% (15643/19200)
Train Batch: 200/352 | Loss: 0.551 | Acc: 81.31% (20815/25600)
Train Batch: 250/352 | Loss: 0.548 | Acc: 81.42% (26053/32000)
Train Batch: 300/352 | Loss: 0.547 | Acc: 81.46% (31279/38400)
Train Batch: 350/352 | Loss: 0.548 | Acc: 81.42% (36476/44800)
Train Batch: 352/352 | Loss: 0.548 | Acc: 81.43% (36642/45000)
Validation | Loss: 0.573 | Acc: 80.78% (4039/5000)

Epoch 25/200
Train Batch: 50/352 | Loss: 0.523 | Acc: 82.73% (5295/6400)
Train Batch: 100/352 | Loss: 0.526 | Acc: 82.62% (10576/12800)
Train Batch: 150/352 | Loss: 0.533 | Acc: 82.23% (15788/19200)
Train Batch: 200/352 | Loss: 0.546 | Acc: 81.80% (20940/25600)
Train Batch: 250/352 | Loss: 0.548 | Acc: 81.61% (26116/32000)
Train Batch: 300/352 | Loss: 0.548 | Acc: 81.61% (31340/38400)
Train Batch: 350/352 | Loss: 0.553 | Acc: 81.42% (36475/44800)
Train Batch: 352/352 | Loss: 0.552 | Acc: 81.42% (36639/45000)
Validation | Loss: 0.547 | Acc: 81.88% (4094/5000)

Epoch 26/200
Train Batch: 50/352 | Loss: 0.531 | Acc: 81.94% (5244/6400)
Train Batch: 100/352 | Loss: 0.543 | Acc: 81.59% (10443/12800)
Train Batch: 150/352 | Loss: 0.536 | Acc: 81.80% (15705/19200)
Train Batch: 200/352 | Loss: 0.542 | Acc: 81.59% (20888/25600)
Train Batch: 250/352 | Loss: 0.542 | Acc: 81.62% (26120/32000)
Train Batch: 300/352 | Loss: 0.544 | Acc: 81.53% (31306/38400)
Train Batch: 350/352 | Loss: 0.544 | Acc: 81.54% (36530/44800)
Train Batch: 352/352 | Loss: 0.544 | Acc: 81.55% (36696/45000)
Validation | Loss: 0.467 | Acc: 84.20% (4210/5000)
New best model with accuracy: 0.8420

Epoch 27/200
Train Batch: 50/352 | Loss: 0.553 | Acc: 81.58% (5221/6400)
Train Batch: 100/352 | Loss: 0.544 | Acc: 81.77% (10466/12800)
Train Batch: 150/352 | Loss: 0.538 | Acc: 81.96% (15736/19200)
Train Batch: 200/352 | Loss: 0.533 | Acc: 82.17% (21036/25600)
Train Batch: 250/352 | Loss: 0.544 | Acc: 81.83% (26184/32000)
Train Batch: 300/352 | Loss: 0.540 | Acc: 81.90% (31450/38400)
Train Batch: 350/352 | Loss: 0.542 | Acc: 81.85% (36671/44800)
Train Batch: 352/352 | Loss: 0.542 | Acc: 81.85% (36834/45000)
Validation | Loss: 0.450 | Acc: 84.46% (4223/5000)
New best model with accuracy: 0.8446

Epoch 28/200
Train Batch: 50/352 | Loss: 0.537 | Acc: 82.00% (5248/6400)
Train Batch: 100/352 | Loss: 0.542 | Acc: 81.68% (10455/12800)
Train Batch: 150/352 | Loss: 0.537 | Acc: 82.01% (15746/19200)
Train Batch: 200/352 | Loss: 0.535 | Acc: 82.00% (20993/25600)
Train Batch: 250/352 | Loss: 0.533 | Acc: 82.12% (26278/32000)
Train Batch: 300/352 | Loss: 0.538 | Acc: 81.96% (31472/38400)
Train Batch: 350/352 | Loss: 0.535 | Acc: 82.04% (36756/44800)
Train Batch: 352/352 | Loss: 0.535 | Acc: 82.05% (36921/45000)
Validation | Loss: 0.431 | Acc: 85.12% (4256/5000)
New best model with accuracy: 0.8512

Epoch 29/200
Train Batch: 50/352 | Loss: 0.521 | Acc: 82.11% (5255/6400)
Train Batch: 100/352 | Loss: 0.534 | Acc: 82.05% (10503/12800)
Train Batch: 150/352 | Loss: 0.537 | Acc: 82.04% (15752/19200)
Train Batch: 200/352 | Loss: 0.536 | Acc: 82.04% (21003/25600)
Train Batch: 250/352 | Loss: 0.535 | Acc: 82.11% (26274/32000)
Train Batch: 300/352 | Loss: 0.536 | Acc: 82.09% (31521/38400)
Train Batch: 350/352 | Loss: 0.533 | Acc: 82.21% (36831/44800)
Train Batch: 352/352 | Loss: 0.533 | Acc: 82.20% (36991/45000)
Validation | Loss: 0.508 | Acc: 82.52% (4126/5000)

Epoch 30/200
Train Batch: 50/352 | Loss: 0.497 | Acc: 83.50% (5344/6400)
Train Batch: 100/352 | Loss: 0.500 | Acc: 83.36% (10670/12800)
Train Batch: 150/352 | Loss: 0.511 | Acc: 82.75% (15888/19200)
Train Batch: 200/352 | Loss: 0.520 | Acc: 82.45% (21107/25600)
Train Batch: 250/352 | Loss: 0.523 | Acc: 82.37% (26359/32000)
Train Batch: 300/352 | Loss: 0.524 | Acc: 82.41% (31646/38400)
Train Batch: 350/352 | Loss: 0.524 | Acc: 82.41% (36918/44800)
Train Batch: 352/352 | Loss: 0.524 | Acc: 82.41% (37084/45000)
Validation | Loss: 0.459 | Acc: 84.38% (4219/5000)

Epoch 31/200
Train Batch: 50/352 | Loss: 0.506 | Acc: 83.39% (5337/6400)
Train Batch: 100/352 | Loss: 0.501 | Acc: 83.35% (10669/12800)
Train Batch: 150/352 | Loss: 0.513 | Acc: 82.94% (15925/19200)
Train Batch: 200/352 | Loss: 0.516 | Acc: 82.76% (21187/25600)
Train Batch: 250/352 | Loss: 0.515 | Acc: 82.72% (26469/32000)
Train Batch: 300/352 | Loss: 0.519 | Acc: 82.56% (31702/38400)
Train Batch: 350/352 | Loss: 0.519 | Acc: 82.45% (36939/44800)
Train Batch: 352/352 | Loss: 0.519 | Acc: 82.46% (37109/45000)
Validation | Loss: 0.468 | Acc: 84.16% (4208/5000)

Epoch 32/200
Train Batch: 50/352 | Loss: 0.500 | Acc: 83.14% (5321/6400)
Train Batch: 100/352 | Loss: 0.497 | Acc: 83.28% (10660/12800)
Train Batch: 150/352 | Loss: 0.498 | Acc: 83.22% (15978/19200)
Train Batch: 200/352 | Loss: 0.505 | Acc: 82.91% (21224/25600)
Train Batch: 250/352 | Loss: 0.512 | Acc: 82.62% (26440/32000)
Train Batch: 300/352 | Loss: 0.515 | Acc: 82.63% (31729/38400)
Train Batch: 350/352 | Loss: 0.517 | Acc: 82.63% (37020/44800)
Train Batch: 352/352 | Loss: 0.517 | Acc: 82.62% (37180/45000)
Validation | Loss: 0.464 | Acc: 84.24% (4212/5000)

Epoch 33/200
Train Batch: 50/352 | Loss: 0.497 | Acc: 82.95% (5309/6400)
Train Batch: 100/352 | Loss: 0.507 | Acc: 82.93% (10615/12800)
Train Batch: 150/352 | Loss: 0.514 | Acc: 82.59% (15858/19200)
Train Batch: 200/352 | Loss: 0.515 | Acc: 82.49% (21117/25600)
Train Batch: 250/352 | Loss: 0.520 | Acc: 82.40% (26367/32000)
Train Batch: 300/352 | Loss: 0.518 | Acc: 82.44% (31658/38400)
Train Batch: 350/352 | Loss: 0.519 | Acc: 82.39% (36910/44800)
Train Batch: 352/352 | Loss: 0.518 | Acc: 82.40% (37081/45000)
Validation | Loss: 0.491 | Acc: 82.50% (4125/5000)

Epoch 34/200
Train Batch: 50/352 | Loss: 0.520 | Acc: 82.31% (5268/6400)
Train Batch: 100/352 | Loss: 0.510 | Acc: 82.86% (10606/12800)
Train Batch: 150/352 | Loss: 0.507 | Acc: 82.87% (15911/19200)
Train Batch: 200/352 | Loss: 0.512 | Acc: 82.80% (21198/25600)
Train Batch: 250/352 | Loss: 0.515 | Acc: 82.71% (26468/32000)
Train Batch: 300/352 | Loss: 0.516 | Acc: 82.65% (31737/38400)
Train Batch: 350/352 | Loss: 0.513 | Acc: 82.85% (37115/44800)
Train Batch: 352/352 | Loss: 0.513 | Acc: 82.83% (37275/45000)
Validation | Loss: 0.473 | Acc: 83.80% (4190/5000)

Epoch 35/200
Train Batch: 50/352 | Loss: 0.490 | Acc: 84.19% (5388/6400)
Train Batch: 100/352 | Loss: 0.497 | Acc: 83.65% (10707/12800)
Train Batch: 150/352 | Loss: 0.490 | Acc: 83.77% (16084/19200)
Train Batch: 200/352 | Loss: 0.497 | Acc: 83.35% (21338/25600)
Train Batch: 250/352 | Loss: 0.502 | Acc: 83.24% (26638/32000)
Train Batch: 300/352 | Loss: 0.502 | Acc: 83.20% (31948/38400)
Train Batch: 350/352 | Loss: 0.505 | Acc: 83.09% (37225/44800)
Train Batch: 352/352 | Loss: 0.505 | Acc: 83.08% (37388/45000)
Validation | Loss: 0.442 | Acc: 84.64% (4232/5000)

Epoch 36/200
Train Batch: 50/352 | Loss: 0.477 | Acc: 84.06% (5380/6400)
Train Batch: 100/352 | Loss: 0.488 | Acc: 83.41% (10677/12800)
Train Batch: 150/352 | Loss: 0.491 | Acc: 83.16% (15967/19200)
Train Batch: 200/352 | Loss: 0.497 | Acc: 83.12% (21279/25600)
Train Batch: 250/352 | Loss: 0.497 | Acc: 83.17% (26613/32000)
Train Batch: 300/352 | Loss: 0.497 | Acc: 83.20% (31947/38400)
Train Batch: 350/352 | Loss: 0.500 | Acc: 83.09% (37225/44800)
Train Batch: 352/352 | Loss: 0.500 | Acc: 83.09% (37389/45000)
Validation | Loss: 0.410 | Acc: 85.86% (4293/5000)
New best model with accuracy: 0.8586

Epoch 37/200
Train Batch: 50/352 | Loss: 0.463 | Acc: 84.06% (5380/6400)
Train Batch: 100/352 | Loss: 0.461 | Acc: 84.25% (10784/12800)
Train Batch: 150/352 | Loss: 0.479 | Acc: 83.70% (16070/19200)
Train Batch: 200/352 | Loss: 0.490 | Acc: 83.28% (21319/25600)
Train Batch: 250/352 | Loss: 0.492 | Acc: 83.21% (26626/32000)
Train Batch: 300/352 | Loss: 0.498 | Acc: 83.08% (31902/38400)
Train Batch: 350/352 | Loss: 0.498 | Acc: 83.02% (37193/44800)
Train Batch: 352/352 | Loss: 0.498 | Acc: 83.03% (37365/45000)
Validation | Loss: 0.413 | Acc: 85.72% (4286/5000)

Epoch 38/200
Train Batch: 50/352 | Loss: 0.457 | Acc: 84.84% (5430/6400)
Train Batch: 100/352 | Loss: 0.472 | Acc: 84.07% (10761/12800)
Train Batch: 150/352 | Loss: 0.486 | Acc: 83.64% (16059/19200)
Train Batch: 200/352 | Loss: 0.487 | Acc: 83.65% (21414/25600)
Train Batch: 250/352 | Loss: 0.489 | Acc: 83.56% (26738/32000)
Train Batch: 300/352 | Loss: 0.490 | Acc: 83.50% (32065/38400)
Train Batch: 350/352 | Loss: 0.490 | Acc: 83.52% (37419/44800)
Train Batch: 352/352 | Loss: 0.491 | Acc: 83.52% (37584/45000)
Validation | Loss: 0.436 | Acc: 84.76% (4238/5000)

Epoch 39/200
Train Batch: 50/352 | Loss: 0.485 | Acc: 83.45% (5341/6400)
Train Batch: 100/352 | Loss: 0.497 | Acc: 83.13% (10641/12800)
Train Batch: 150/352 | Loss: 0.501 | Acc: 83.00% (15936/19200)
Train Batch: 200/352 | Loss: 0.496 | Acc: 83.18% (21293/25600)
Train Batch: 250/352 | Loss: 0.501 | Acc: 83.05% (26576/32000)
Train Batch: 300/352 | Loss: 0.499 | Acc: 83.10% (31912/38400)
Train Batch: 350/352 | Loss: 0.499 | Acc: 83.11% (37232/44800)
Train Batch: 352/352 | Loss: 0.498 | Acc: 83.12% (37405/45000)
Validation | Loss: 0.407 | Acc: 86.30% (4315/5000)
New best model with accuracy: 0.8630

Epoch 40/200
Train Batch: 50/352 | Loss: 0.488 | Acc: 83.47% (5342/6400)
Train Batch: 100/352 | Loss: 0.489 | Acc: 83.55% (10694/12800)
Train Batch: 150/352 | Loss: 0.492 | Acc: 83.51% (16033/19200)
Train Batch: 200/352 | Loss: 0.493 | Acc: 83.57% (21394/25600)
Train Batch: 250/352 | Loss: 0.490 | Acc: 83.60% (26753/32000)
Train Batch: 300/352 | Loss: 0.493 | Acc: 83.46% (32048/38400)
Train Batch: 350/352 | Loss: 0.497 | Acc: 83.27% (37305/44800)
Train Batch: 352/352 | Loss: 0.497 | Acc: 83.27% (37471/45000)
Validation | Loss: 0.509 | Acc: 83.18% (4159/5000)

Epoch 41/200
Train Batch: 50/352 | Loss: 0.489 | Acc: 83.34% (5334/6400)
Train Batch: 100/352 | Loss: 0.489 | Acc: 83.62% (10703/12800)
Train Batch: 150/352 | Loss: 0.494 | Acc: 83.43% (16019/19200)
Train Batch: 200/352 | Loss: 0.490 | Acc: 83.54% (21385/25600)
Train Batch: 250/352 | Loss: 0.490 | Acc: 83.55% (26735/32000)
Train Batch: 300/352 | Loss: 0.492 | Acc: 83.46% (32050/38400)
Train Batch: 350/352 | Loss: 0.490 | Acc: 83.55% (37431/44800)
Train Batch: 352/352 | Loss: 0.490 | Acc: 83.54% (37595/45000)
Validation | Loss: 0.514 | Acc: 83.06% (4153/5000)

Epoch 42/200
Train Batch: 50/352 | Loss: 0.470 | Acc: 84.47% (5406/6400)
Train Batch: 100/352 | Loss: 0.469 | Acc: 84.42% (10806/12800)
Train Batch: 150/352 | Loss: 0.475 | Acc: 84.23% (16172/19200)
Train Batch: 200/352 | Loss: 0.479 | Acc: 84.02% (21509/25600)
Train Batch: 250/352 | Loss: 0.482 | Acc: 83.96% (26866/32000)
Train Batch: 300/352 | Loss: 0.484 | Acc: 83.87% (32207/38400)
Train Batch: 350/352 | Loss: 0.486 | Acc: 83.73% (37509/44800)
Train Batch: 352/352 | Loss: 0.486 | Acc: 83.71% (37671/45000)
Validation | Loss: 0.429 | Acc: 85.02% (4251/5000)

Epoch 43/200
Train Batch: 50/352 | Loss: 0.485 | Acc: 83.47% (5342/6400)
Train Batch: 100/352 | Loss: 0.483 | Acc: 83.75% (10720/12800)
Train Batch: 150/352 | Loss: 0.490 | Acc: 83.64% (16059/19200)
Train Batch: 200/352 | Loss: 0.484 | Acc: 83.82% (21458/25600)
Train Batch: 250/352 | Loss: 0.483 | Acc: 83.82% (26821/32000)
Train Batch: 300/352 | Loss: 0.485 | Acc: 83.72% (32150/38400)
Train Batch: 350/352 | Loss: 0.484 | Acc: 83.78% (37535/44800)
Train Batch: 352/352 | Loss: 0.483 | Acc: 83.81% (37713/45000)
Validation | Loss: 0.453 | Acc: 84.16% (4208/5000)

Epoch 44/200
Train Batch: 50/352 | Loss: 0.464 | Acc: 84.11% (5383/6400)
Train Batch: 100/352 | Loss: 0.465 | Acc: 84.05% (10758/12800)
Train Batch: 150/352 | Loss: 0.472 | Acc: 83.94% (16117/19200)
Train Batch: 200/352 | Loss: 0.476 | Acc: 83.84% (21462/25600)
Train Batch: 250/352 | Loss: 0.476 | Acc: 83.77% (26807/32000)
Train Batch: 300/352 | Loss: 0.480 | Acc: 83.73% (32151/38400)
Train Batch: 350/352 | Loss: 0.478 | Acc: 83.77% (37529/44800)
Train Batch: 352/352 | Loss: 0.479 | Acc: 83.76% (37694/45000)
Validation | Loss: 0.414 | Acc: 85.50% (4275/5000)

Epoch 45/200
Train Batch: 50/352 | Loss: 0.472 | Acc: 84.44% (5404/6400)
Train Batch: 100/352 | Loss: 0.469 | Acc: 84.29% (10789/12800)
Train Batch: 150/352 | Loss: 0.470 | Acc: 84.14% (16154/19200)
Train Batch: 200/352 | Loss: 0.473 | Acc: 84.11% (21533/25600)
Train Batch: 250/352 | Loss: 0.473 | Acc: 84.06% (26899/32000)
Train Batch: 300/352 | Loss: 0.476 | Acc: 84.04% (32271/38400)
Train Batch: 350/352 | Loss: 0.479 | Acc: 83.92% (37598/44800)
Train Batch: 352/352 | Loss: 0.479 | Acc: 83.92% (37765/45000)
Validation | Loss: 0.438 | Acc: 84.94% (4247/5000)

Epoch 46/200
Train Batch: 50/352 | Loss: 0.458 | Acc: 84.64% (5417/6400)
Train Batch: 100/352 | Loss: 0.460 | Acc: 84.46% (10811/12800)
Train Batch: 150/352 | Loss: 0.461 | Acc: 84.45% (16215/19200)
Train Batch: 200/352 | Loss: 0.472 | Acc: 84.18% (21551/25600)
Train Batch: 250/352 | Loss: 0.477 | Acc: 84.00% (26880/32000)
Train Batch: 300/352 | Loss: 0.477 | Acc: 83.93% (32229/38400)
Train Batch: 350/352 | Loss: 0.480 | Acc: 83.85% (37566/44800)
Train Batch: 352/352 | Loss: 0.480 | Acc: 83.85% (37732/45000)
Validation | Loss: 0.454 | Acc: 84.28% (4214/5000)

Epoch 47/200
Train Batch: 50/352 | Loss: 0.458 | Acc: 84.66% (5418/6400)
Train Batch: 100/352 | Loss: 0.467 | Acc: 84.30% (10791/12800)
Train Batch: 150/352 | Loss: 0.471 | Acc: 84.19% (16164/19200)
Train Batch: 200/352 | Loss: 0.468 | Acc: 84.28% (21575/25600)
Train Batch: 250/352 | Loss: 0.474 | Acc: 84.07% (26901/32000)
Train Batch: 300/352 | Loss: 0.477 | Acc: 83.87% (32205/38400)
Train Batch: 350/352 | Loss: 0.474 | Acc: 83.99% (37628/44800)
Train Batch: 352/352 | Loss: 0.475 | Acc: 83.97% (37785/45000)
Validation | Loss: 0.427 | Acc: 85.16% (4258/5000)

Epoch 48/200
Train Batch: 50/352 | Loss: 0.445 | Acc: 84.92% (5435/6400)
Train Batch: 100/352 | Loss: 0.456 | Acc: 84.88% (10865/12800)
Train Batch: 150/352 | Loss: 0.455 | Acc: 84.89% (16299/19200)
Train Batch: 200/352 | Loss: 0.463 | Acc: 84.62% (21662/25600)
Train Batch: 250/352 | Loss: 0.464 | Acc: 84.50% (27039/32000)
Train Batch: 300/352 | Loss: 0.464 | Acc: 84.47% (32437/38400)
Train Batch: 350/352 | Loss: 0.467 | Acc: 84.37% (37799/44800)
Train Batch: 352/352 | Loss: 0.467 | Acc: 84.37% (37968/45000)
Validation | Loss: 0.430 | Acc: 85.48% (4274/5000)

Epoch 49/200
Train Batch: 50/352 | Loss: 0.465 | Acc: 84.44% (5404/6400)
Train Batch: 100/352 | Loss: 0.466 | Acc: 84.52% (10819/12800)
Train Batch: 150/352 | Loss: 0.464 | Acc: 84.58% (16240/19200)
Train Batch: 200/352 | Loss: 0.463 | Acc: 84.58% (21652/25600)
Train Batch: 250/352 | Loss: 0.467 | Acc: 84.41% (27012/32000)
Train Batch: 300/352 | Loss: 0.465 | Acc: 84.40% (32408/38400)
Train Batch: 350/352 | Loss: 0.469 | Acc: 84.33% (37782/44800)
Train Batch: 352/352 | Loss: 0.469 | Acc: 84.33% (37950/45000)
Validation | Loss: 0.529 | Acc: 82.40% (4120/5000)

Epoch 50/200
Train Batch: 50/352 | Loss: 0.479 | Acc: 84.34% (5398/6400)
Train Batch: 100/352 | Loss: 0.470 | Acc: 84.58% (10826/12800)
Train Batch: 150/352 | Loss: 0.475 | Acc: 84.42% (16208/19200)
Train Batch: 200/352 | Loss: 0.471 | Acc: 84.37% (21599/25600)
Train Batch: 250/352 | Loss: 0.467 | Acc: 84.46% (27027/32000)
Train Batch: 300/352 | Loss: 0.465 | Acc: 84.52% (32457/38400)
Train Batch: 350/352 | Loss: 0.468 | Acc: 84.48% (37845/44800)
Train Batch: 352/352 | Loss: 0.468 | Acc: 84.48% (38014/45000)
Validation | Loss: 0.418 | Acc: 85.50% (4275/5000)

Epoch 51/200
Train Batch: 50/352 | Loss: 0.452 | Acc: 84.72% (5422/6400)
Train Batch: 100/352 | Loss: 0.461 | Acc: 84.40% (10803/12800)
Train Batch: 150/352 | Loss: 0.464 | Acc: 84.36% (16197/19200)
Train Batch: 200/352 | Loss: 0.466 | Acc: 84.34% (21590/25600)
Train Batch: 250/352 | Loss: 0.461 | Acc: 84.47% (27032/32000)
Train Batch: 300/352 | Loss: 0.462 | Acc: 84.46% (32431/38400)
Train Batch: 350/352 | Loss: 0.464 | Acc: 84.33% (37778/44800)
Train Batch: 352/352 | Loss: 0.465 | Acc: 84.31% (37941/45000)
Validation | Loss: 0.517 | Acc: 82.70% (4135/5000)

Epoch 52/200
Train Batch: 50/352 | Loss: 0.462 | Acc: 84.19% (5388/6400)
Train Batch: 100/352 | Loss: 0.459 | Acc: 84.66% (10837/12800)
Train Batch: 150/352 | Loss: 0.455 | Acc: 84.78% (16278/19200)
Train Batch: 200/352 | Loss: 0.459 | Acc: 84.77% (21700/25600)
Train Batch: 250/352 | Loss: 0.460 | Acc: 84.71% (27107/32000)
Train Batch: 300/352 | Loss: 0.462 | Acc: 84.68% (32518/38400)
Train Batch: 350/352 | Loss: 0.466 | Acc: 84.50% (37855/44800)
Train Batch: 352/352 | Loss: 0.466 | Acc: 84.50% (38023/45000)
Validation | Loss: 0.449 | Acc: 84.90% (4245/5000)

Epoch 53/200
Train Batch: 50/352 | Loss: 0.430 | Acc: 85.42% (5467/6400)
Train Batch: 100/352 | Loss: 0.441 | Acc: 85.30% (10919/12800)
Train Batch: 150/352 | Loss: 0.450 | Acc: 84.83% (16287/19200)
Train Batch: 200/352 | Loss: 0.450 | Acc: 84.88% (21729/25600)
Train Batch: 250/352 | Loss: 0.456 | Acc: 84.69% (27101/32000)
Train Batch: 300/352 | Loss: 0.458 | Acc: 84.56% (32471/38400)
Train Batch: 350/352 | Loss: 0.460 | Acc: 84.52% (37866/44800)
Train Batch: 352/352 | Loss: 0.460 | Acc: 84.53% (38038/45000)
Validation | Loss: 0.428 | Acc: 85.02% (4251/5000)

Epoch 54/200
Train Batch: 50/352 | Loss: 0.457 | Acc: 84.78% (5426/6400)
Train Batch: 100/352 | Loss: 0.457 | Acc: 84.43% (10807/12800)
Train Batch: 150/352 | Loss: 0.458 | Acc: 84.54% (16232/19200)
Train Batch: 200/352 | Loss: 0.457 | Acc: 84.59% (21656/25600)
Train Batch: 250/352 | Loss: 0.455 | Acc: 84.70% (27104/32000)
Train Batch: 300/352 | Loss: 0.455 | Acc: 84.76% (32548/38400)
Train Batch: 350/352 | Loss: 0.456 | Acc: 84.68% (37936/44800)
Train Batch: 352/352 | Loss: 0.456 | Acc: 84.67% (38103/45000)
Validation | Loss: 0.426 | Acc: 85.38% (4269/5000)

Epoch 55/200
Train Batch: 50/352 | Loss: 0.447 | Acc: 85.06% (5444/6400)
Train Batch: 100/352 | Loss: 0.445 | Acc: 84.98% (10878/12800)
Train Batch: 150/352 | Loss: 0.440 | Acc: 85.18% (16354/19200)
Train Batch: 200/352 | Loss: 0.443 | Acc: 85.02% (21766/25600)
Train Batch: 250/352 | Loss: 0.448 | Acc: 84.91% (27172/32000)
Train Batch: 300/352 | Loss: 0.450 | Acc: 84.81% (32567/38400)
Train Batch: 350/352 | Loss: 0.451 | Acc: 84.74% (37964/44800)
Train Batch: 352/352 | Loss: 0.451 | Acc: 84.75% (38136/45000)
Validation | Loss: 0.396 | Acc: 86.60% (4330/5000)
New best model with accuracy: 0.8660

Epoch 56/200
Train Batch: 50/352 | Loss: 0.461 | Acc: 84.38% (5400/6400)
Train Batch: 100/352 | Loss: 0.444 | Acc: 85.26% (10913/12800)
Train Batch: 150/352 | Loss: 0.450 | Acc: 84.97% (16315/19200)
Train Batch: 200/352 | Loss: 0.452 | Acc: 84.82% (21715/25600)
Train Batch: 250/352 | Loss: 0.447 | Acc: 84.97% (27192/32000)
Train Batch: 300/352 | Loss: 0.444 | Acc: 85.07% (32665/38400)
Train Batch: 350/352 | Loss: 0.448 | Acc: 84.96% (38062/44800)
Train Batch: 352/352 | Loss: 0.448 | Acc: 84.96% (38230/45000)
Validation | Loss: 0.494 | Acc: 83.24% (4162/5000)

Epoch 57/200
Train Batch: 50/352 | Loss: 0.436 | Acc: 85.38% (5464/6400)
Train Batch: 100/352 | Loss: 0.449 | Acc: 84.77% (10851/12800)
Train Batch: 150/352 | Loss: 0.458 | Acc: 84.45% (16215/19200)
Train Batch: 200/352 | Loss: 0.455 | Acc: 84.59% (21654/25600)
Train Batch: 250/352 | Loss: 0.455 | Acc: 84.69% (27102/32000)
Train Batch: 300/352 | Loss: 0.456 | Acc: 84.70% (32523/38400)
Train Batch: 350/352 | Loss: 0.455 | Acc: 84.75% (37966/44800)
Train Batch: 352/352 | Loss: 0.456 | Acc: 84.72% (38125/45000)
Validation | Loss: 0.373 | Acc: 87.14% (4357/5000)
New best model with accuracy: 0.8714

Epoch 58/200
Train Batch: 50/352 | Loss: 0.450 | Acc: 85.12% (5448/6400)
Train Batch: 100/352 | Loss: 0.439 | Acc: 85.34% (10924/12800)
Train Batch: 150/352 | Loss: 0.435 | Acc: 85.37% (16391/19200)
Train Batch: 200/352 | Loss: 0.439 | Acc: 85.32% (21843/25600)
Train Batch: 250/352 | Loss: 0.439 | Acc: 85.14% (27245/32000)
Train Batch: 300/352 | Loss: 0.441 | Acc: 85.09% (32674/38400)
Train Batch: 350/352 | Loss: 0.444 | Acc: 85.02% (38090/44800)
Train Batch: 352/352 | Loss: 0.445 | Acc: 85.02% (38261/45000)
Validation | Loss: 0.368 | Acc: 86.34% (4317/5000)

Epoch 59/200
Train Batch: 50/352 | Loss: 0.433 | Acc: 85.58% (5477/6400)
Train Batch: 100/352 | Loss: 0.441 | Acc: 85.40% (10931/12800)
Train Batch: 150/352 | Loss: 0.444 | Acc: 85.43% (16403/19200)
Train Batch: 200/352 | Loss: 0.443 | Acc: 85.43% (21870/25600)
Train Batch: 250/352 | Loss: 0.444 | Acc: 85.19% (27262/32000)
Train Batch: 300/352 | Loss: 0.441 | Acc: 85.18% (32711/38400)
Train Batch: 350/352 | Loss: 0.441 | Acc: 85.16% (38153/44800)
Train Batch: 352/352 | Loss: 0.441 | Acc: 85.16% (38324/45000)
Validation | Loss: 0.466 | Acc: 84.22% (4211/5000)

Epoch 60/200
Train Batch: 50/352 | Loss: 0.446 | Acc: 84.73% (5423/6400)
Train Batch: 100/352 | Loss: 0.437 | Acc: 85.08% (10890/12800)
Train Batch: 150/352 | Loss: 0.439 | Acc: 84.88% (16297/19200)
Train Batch: 200/352 | Loss: 0.444 | Acc: 84.91% (21736/25600)
Train Batch: 250/352 | Loss: 0.447 | Acc: 84.88% (27163/32000)
Train Batch: 300/352 | Loss: 0.448 | Acc: 84.91% (32606/38400)
Train Batch: 350/352 | Loss: 0.447 | Acc: 85.01% (38083/44800)
Train Batch: 352/352 | Loss: 0.446 | Acc: 85.02% (38260/45000)
Validation | Loss: 0.436 | Acc: 85.68% (4284/5000)

Epoch 61/200
Train Batch: 50/352 | Loss: 0.418 | Acc: 86.16% (5514/6400)
Train Batch: 100/352 | Loss: 0.426 | Acc: 85.70% (10969/12800)
Train Batch: 150/352 | Loss: 0.432 | Acc: 85.49% (16414/19200)
Train Batch: 200/352 | Loss: 0.436 | Acc: 85.36% (21853/25600)
Train Batch: 250/352 | Loss: 0.440 | Acc: 85.27% (27285/32000)
Train Batch: 300/352 | Loss: 0.440 | Acc: 85.28% (32746/38400)
Train Batch: 350/352 | Loss: 0.440 | Acc: 85.31% (38217/44800)
Train Batch: 352/352 | Loss: 0.440 | Acc: 85.30% (38387/45000)
Validation | Loss: 0.374 | Acc: 86.64% (4332/5000)

Epoch 62/200
Train Batch: 50/352 | Loss: 0.442 | Acc: 84.84% (5430/6400)
Train Batch: 100/352 | Loss: 0.440 | Acc: 84.97% (10876/12800)
Train Batch: 150/352 | Loss: 0.437 | Acc: 85.06% (16331/19200)
Train Batch: 200/352 | Loss: 0.439 | Acc: 85.11% (21787/25600)
Train Batch: 250/352 | Loss: 0.439 | Acc: 85.19% (27261/32000)
Train Batch: 300/352 | Loss: 0.440 | Acc: 85.17% (32704/38400)
Train Batch: 350/352 | Loss: 0.439 | Acc: 85.23% (38185/44800)
Train Batch: 352/352 | Loss: 0.439 | Acc: 85.25% (38361/45000)
Validation | Loss: 0.395 | Acc: 86.16% (4308/5000)

Epoch 63/200
Train Batch: 50/352 | Loss: 0.432 | Acc: 85.48% (5471/6400)
Train Batch: 100/352 | Loss: 0.446 | Acc: 84.95% (10873/12800)
Train Batch: 150/352 | Loss: 0.445 | Acc: 84.96% (16312/19200)
Train Batch: 200/352 | Loss: 0.442 | Acc: 85.14% (21797/25600)
Train Batch: 250/352 | Loss: 0.443 | Acc: 85.14% (27244/32000)
Train Batch: 300/352 | Loss: 0.444 | Acc: 85.12% (32688/38400)
Train Batch: 350/352 | Loss: 0.443 | Acc: 85.15% (38147/44800)
Train Batch: 352/352 | Loss: 0.444 | Acc: 85.14% (38312/45000)
Validation | Loss: 0.374 | Acc: 86.76% (4338/5000)

Epoch 64/200
Train Batch: 50/352 | Loss: 0.422 | Acc: 85.94% (5500/6400)
Train Batch: 100/352 | Loss: 0.433 | Acc: 85.46% (10939/12800)
Train Batch: 150/352 | Loss: 0.436 | Acc: 85.35% (16387/19200)
Train Batch: 200/352 | Loss: 0.435 | Acc: 85.37% (21854/25600)
Train Batch: 250/352 | Loss: 0.439 | Acc: 85.31% (27298/32000)
Train Batch: 300/352 | Loss: 0.439 | Acc: 85.26% (32740/38400)
Train Batch: 350/352 | Loss: 0.434 | Acc: 85.52% (38313/44800)
Train Batch: 352/352 | Loss: 0.434 | Acc: 85.53% (38489/45000)
Validation | Loss: 0.353 | Acc: 87.94% (4397/5000)
New best model with accuracy: 0.8794

Epoch 65/200
Train Batch: 50/352 | Loss: 0.411 | Acc: 86.44% (5532/6400)
Train Batch: 100/352 | Loss: 0.413 | Acc: 86.23% (11038/12800)
Train Batch: 150/352 | Loss: 0.421 | Acc: 86.07% (16525/19200)
Train Batch: 200/352 | Loss: 0.426 | Acc: 85.82% (21970/25600)
Train Batch: 250/352 | Loss: 0.427 | Acc: 85.73% (27435/32000)
Train Batch: 300/352 | Loss: 0.432 | Acc: 85.61% (32874/38400)
Train Batch: 350/352 | Loss: 0.437 | Acc: 85.43% (38274/44800)
Train Batch: 352/352 | Loss: 0.436 | Acc: 85.44% (38447/45000)
Validation | Loss: 0.389 | Acc: 86.38% (4319/5000)

Epoch 66/200
Train Batch: 50/352 | Loss: 0.407 | Acc: 86.23% (5519/6400)
Train Batch: 100/352 | Loss: 0.421 | Acc: 85.98% (11006/12800)
Train Batch: 150/352 | Loss: 0.420 | Acc: 86.08% (16527/19200)
Train Batch: 200/352 | Loss: 0.425 | Acc: 85.84% (21974/25600)
Train Batch: 250/352 | Loss: 0.428 | Acc: 85.68% (27418/32000)
Train Batch: 300/352 | Loss: 0.428 | Acc: 85.69% (32906/38400)
Train Batch: 350/352 | Loss: 0.427 | Acc: 85.76% (38421/44800)
Train Batch: 352/352 | Loss: 0.428 | Acc: 85.74% (38585/45000)
Validation | Loss: 0.357 | Acc: 87.78% (4389/5000)

Epoch 67/200
Train Batch: 50/352 | Loss: 0.447 | Acc: 85.17% (5451/6400)
Train Batch: 100/352 | Loss: 0.428 | Acc: 85.64% (10962/12800)
Train Batch: 150/352 | Loss: 0.432 | Acc: 85.38% (16393/19200)
Train Batch: 200/352 | Loss: 0.432 | Acc: 85.34% (21848/25600)
Train Batch: 250/352 | Loss: 0.433 | Acc: 85.43% (27338/32000)
Train Batch: 300/352 | Loss: 0.434 | Acc: 85.32% (32763/38400)
Train Batch: 350/352 | Loss: 0.434 | Acc: 85.36% (38242/44800)
Train Batch: 352/352 | Loss: 0.434 | Acc: 85.34% (38403/45000)
Validation | Loss: 0.429 | Acc: 86.02% (4301/5000)

Epoch 68/200
Train Batch: 50/352 | Loss: 0.407 | Acc: 86.09% (5510/6400)
Train Batch: 100/352 | Loss: 0.414 | Acc: 85.92% (10998/12800)
Train Batch: 150/352 | Loss: 0.414 | Acc: 85.83% (16479/19200)
Train Batch: 200/352 | Loss: 0.420 | Acc: 85.62% (21920/25600)
Train Batch: 250/352 | Loss: 0.419 | Acc: 85.76% (27443/32000)
Train Batch: 300/352 | Loss: 0.416 | Acc: 85.92% (32993/38400)
Train Batch: 350/352 | Loss: 0.419 | Acc: 85.78% (38428/44800)
Train Batch: 352/352 | Loss: 0.419 | Acc: 85.79% (38604/45000)
Validation | Loss: 0.377 | Acc: 86.98% (4349/5000)

Epoch 69/200
Train Batch: 50/352 | Loss: 0.415 | Acc: 86.48% (5535/6400)
Train Batch: 100/352 | Loss: 0.418 | Acc: 86.17% (11030/12800)
Train Batch: 150/352 | Loss: 0.417 | Acc: 86.08% (16527/19200)
Train Batch: 200/352 | Loss: 0.422 | Acc: 85.91% (21992/25600)
Train Batch: 250/352 | Loss: 0.422 | Acc: 85.94% (27500/32000)
Train Batch: 300/352 | Loss: 0.421 | Acc: 85.91% (32989/38400)
Train Batch: 350/352 | Loss: 0.425 | Acc: 85.83% (38454/44800)
Train Batch: 352/352 | Loss: 0.425 | Acc: 85.83% (38623/45000)
Validation | Loss: 0.366 | Acc: 87.56% (4378/5000)

Epoch 70/200
Train Batch: 50/352 | Loss: 0.410 | Acc: 86.25% (5520/6400)
Train Batch: 100/352 | Loss: 0.416 | Acc: 85.83% (10986/12800)
Train Batch: 150/352 | Loss: 0.411 | Acc: 86.03% (16517/19200)
Train Batch: 200/352 | Loss: 0.413 | Acc: 86.02% (22020/25600)
Train Batch: 250/352 | Loss: 0.419 | Acc: 85.84% (27470/32000)
Train Batch: 300/352 | Loss: 0.418 | Acc: 85.92% (32995/38400)
Train Batch: 350/352 | Loss: 0.418 | Acc: 85.88% (38476/44800)
Train Batch: 352/352 | Loss: 0.418 | Acc: 85.90% (38655/45000)
Validation | Loss: 0.446 | Acc: 84.82% (4241/5000)

Epoch 71/200
Train Batch: 50/352 | Loss: 0.433 | Acc: 85.64% (5481/6400)
Train Batch: 100/352 | Loss: 0.420 | Acc: 86.09% (11019/12800)
Train Batch: 150/352 | Loss: 0.415 | Acc: 86.39% (16586/19200)
Train Batch: 200/352 | Loss: 0.416 | Acc: 86.21% (22071/25600)
Train Batch: 250/352 | Loss: 0.417 | Acc: 86.16% (27571/32000)
Train Batch: 300/352 | Loss: 0.420 | Acc: 86.07% (33050/38400)
Train Batch: 350/352 | Loss: 0.421 | Acc: 85.99% (38522/44800)
Train Batch: 352/352 | Loss: 0.421 | Acc: 86.01% (38703/45000)
Validation | Loss: 0.360 | Acc: 87.70% (4385/5000)

Epoch 72/200
Train Batch: 50/352 | Loss: 0.416 | Acc: 85.91% (5498/6400)
Train Batch: 100/352 | Loss: 0.418 | Acc: 85.86% (10990/12800)
Train Batch: 150/352 | Loss: 0.414 | Acc: 86.12% (16535/19200)
Train Batch: 200/352 | Loss: 0.411 | Acc: 86.26% (22083/25600)
Train Batch: 250/352 | Loss: 0.415 | Acc: 86.21% (27588/32000)
Train Batch: 300/352 | Loss: 0.418 | Acc: 86.13% (33073/38400)
Train Batch: 350/352 | Loss: 0.419 | Acc: 86.06% (38553/44800)
Train Batch: 352/352 | Loss: 0.420 | Acc: 86.04% (38720/45000)
Validation | Loss: 0.339 | Acc: 88.38% (4419/5000)
New best model with accuracy: 0.8838

Epoch 73/200
Train Batch: 50/352 | Loss: 0.383 | Acc: 87.11% (5575/6400)
Train Batch: 100/352 | Loss: 0.393 | Acc: 86.82% (11113/12800)
Train Batch: 150/352 | Loss: 0.395 | Acc: 86.76% (16658/19200)
Train Batch: 200/352 | Loss: 0.400 | Acc: 86.54% (22153/25600)
Train Batch: 250/352 | Loss: 0.400 | Acc: 86.52% (27686/32000)
Train Batch: 300/352 | Loss: 0.405 | Acc: 86.41% (33181/38400)
Train Batch: 350/352 | Loss: 0.407 | Acc: 86.31% (38669/44800)
Train Batch: 352/352 | Loss: 0.407 | Acc: 86.32% (38845/45000)
Validation | Loss: 0.394 | Acc: 86.50% (4325/5000)

Epoch 74/200
Train Batch: 50/352 | Loss: 0.398 | Acc: 86.56% (5540/6400)
Train Batch: 100/352 | Loss: 0.396 | Acc: 86.58% (11082/12800)
Train Batch: 150/352 | Loss: 0.409 | Acc: 86.30% (16570/19200)
Train Batch: 200/352 | Loss: 0.411 | Acc: 86.25% (22080/25600)
Train Batch: 250/352 | Loss: 0.413 | Acc: 86.12% (27560/32000)
Train Batch: 300/352 | Loss: 0.412 | Acc: 86.11% (33065/38400)
Train Batch: 350/352 | Loss: 0.414 | Acc: 86.05% (38551/44800)
Train Batch: 352/352 | Loss: 0.415 | Acc: 86.04% (38718/45000)
Validation | Loss: 0.375 | Acc: 87.18% (4359/5000)

Epoch 75/200
Train Batch: 50/352 | Loss: 0.400 | Acc: 86.66% (5546/6400)
Train Batch: 100/352 | Loss: 0.399 | Acc: 86.52% (11075/12800)
Train Batch: 150/352 | Loss: 0.399 | Acc: 86.55% (16617/19200)
Train Batch: 200/352 | Loss: 0.401 | Acc: 86.46% (22135/25600)
Train Batch: 250/352 | Loss: 0.401 | Acc: 86.46% (27668/32000)
Train Batch: 300/352 | Loss: 0.404 | Acc: 86.41% (33180/38400)
Train Batch: 350/352 | Loss: 0.405 | Acc: 86.41% (38710/44800)
Train Batch: 352/352 | Loss: 0.405 | Acc: 86.40% (38881/45000)
Validation | Loss: 0.391 | Acc: 86.56% (4328/5000)

Epoch 76/200
Train Batch: 50/352 | Loss: 0.389 | Acc: 86.84% (5558/6400)
Train Batch: 100/352 | Loss: 0.386 | Acc: 86.92% (11126/12800)
Train Batch: 150/352 | Loss: 0.395 | Acc: 86.71% (16649/19200)
Train Batch: 200/352 | Loss: 0.399 | Acc: 86.55% (22156/25600)
Train Batch: 250/352 | Loss: 0.401 | Acc: 86.52% (27686/32000)
Train Batch: 300/352 | Loss: 0.396 | Acc: 86.72% (33301/38400)
Train Batch: 350/352 | Loss: 0.403 | Acc: 86.46% (38734/44800)
Train Batch: 352/352 | Loss: 0.403 | Acc: 86.47% (38910/45000)
Validation | Loss: 0.362 | Acc: 87.64% (4382/5000)

Epoch 77/200
Train Batch: 50/352 | Loss: 0.407 | Acc: 86.30% (5523/6400)
Train Batch: 100/352 | Loss: 0.402 | Acc: 86.40% (11059/12800)
Train Batch: 150/352 | Loss: 0.406 | Acc: 86.20% (16551/19200)
Train Batch: 200/352 | Loss: 0.406 | Acc: 86.30% (22092/25600)
Train Batch: 250/352 | Loss: 0.405 | Acc: 86.39% (27646/32000)
Train Batch: 300/352 | Loss: 0.405 | Acc: 86.33% (33149/38400)
Train Batch: 350/352 | Loss: 0.405 | Acc: 86.36% (38689/44800)
Train Batch: 352/352 | Loss: 0.406 | Acc: 86.35% (38857/45000)
Validation | Loss: 0.405 | Acc: 85.90% (4295/5000)

Epoch 78/200
Train Batch: 50/352 | Loss: 0.387 | Acc: 87.11% (5575/6400)
Train Batch: 100/352 | Loss: 0.378 | Acc: 87.21% (11163/12800)
Train Batch: 150/352 | Loss: 0.380 | Acc: 87.15% (16732/19200)
Train Batch: 200/352 | Loss: 0.387 | Acc: 87.03% (22280/25600)
Train Batch: 250/352 | Loss: 0.390 | Acc: 86.95% (27824/32000)
Train Batch: 300/352 | Loss: 0.392 | Acc: 86.84% (33348/38400)
Train Batch: 350/352 | Loss: 0.394 | Acc: 86.75% (38863/44800)
Train Batch: 352/352 | Loss: 0.395 | Acc: 86.76% (39040/45000)
Validation | Loss: 0.362 | Acc: 87.00% (4350/5000)

Epoch 79/200
Train Batch: 50/352 | Loss: 0.375 | Acc: 87.73% (5615/6400)
Train Batch: 100/352 | Loss: 0.389 | Acc: 87.08% (11146/12800)
Train Batch: 150/352 | Loss: 0.394 | Acc: 86.91% (16687/19200)
Train Batch: 200/352 | Loss: 0.394 | Acc: 86.91% (22248/25600)
Train Batch: 250/352 | Loss: 0.393 | Acc: 86.96% (27828/32000)
Train Batch: 300/352 | Loss: 0.397 | Acc: 86.78% (33323/38400)
Train Batch: 350/352 | Loss: 0.398 | Acc: 86.74% (38860/44800)
Train Batch: 352/352 | Loss: 0.398 | Acc: 86.74% (39034/45000)
Validation | Loss: 0.472 | Acc: 83.86% (4193/5000)

Epoch 80/200
Train Batch: 50/352 | Loss: 0.380 | Acc: 87.09% (5574/6400)
Train Batch: 100/352 | Loss: 0.389 | Acc: 86.90% (11123/12800)
Train Batch: 150/352 | Loss: 0.395 | Acc: 86.58% (16623/19200)
Train Batch: 200/352 | Loss: 0.400 | Acc: 86.37% (22110/25600)
Train Batch: 250/352 | Loss: 0.393 | Acc: 86.62% (27718/32000)
Train Batch: 300/352 | Loss: 0.394 | Acc: 86.66% (33276/38400)
Train Batch: 350/352 | Loss: 0.393 | Acc: 86.72% (38849/44800)
Train Batch: 352/352 | Loss: 0.393 | Acc: 86.72% (39025/45000)
Validation | Loss: 0.374 | Acc: 87.16% (4358/5000)

Epoch 81/200
Train Batch: 50/352 | Loss: 0.370 | Acc: 87.56% (5604/6400)
Train Batch: 100/352 | Loss: 0.383 | Acc: 86.95% (11129/12800)
Train Batch: 150/352 | Loss: 0.386 | Acc: 86.85% (16675/19200)
Train Batch: 200/352 | Loss: 0.389 | Acc: 86.82% (22225/25600)
Train Batch: 250/352 | Loss: 0.386 | Acc: 86.91% (27811/32000)
Train Batch: 300/352 | Loss: 0.389 | Acc: 86.86% (33356/38400)
Train Batch: 350/352 | Loss: 0.392 | Acc: 86.78% (38878/44800)
Train Batch: 352/352 | Loss: 0.393 | Acc: 86.78% (39050/45000)
Validation | Loss: 0.344 | Acc: 87.68% (4384/5000)

Epoch 82/200
Train Batch: 50/352 | Loss: 0.377 | Acc: 87.58% (5605/6400)
Train Batch: 100/352 | Loss: 0.391 | Acc: 87.12% (11151/12800)
Train Batch: 150/352 | Loss: 0.389 | Acc: 87.01% (16705/19200)
Train Batch: 200/352 | Loss: 0.385 | Acc: 87.12% (22302/25600)
Train Batch: 250/352 | Loss: 0.383 | Acc: 87.12% (27880/32000)
Train Batch: 300/352 | Loss: 0.384 | Acc: 87.05% (33428/38400)
Train Batch: 350/352 | Loss: 0.387 | Acc: 86.93% (38946/44800)
Train Batch: 352/352 | Loss: 0.387 | Acc: 86.93% (39119/45000)
Validation | Loss: 0.305 | Acc: 89.38% (4469/5000)
New best model with accuracy: 0.8938

Epoch 83/200
Train Batch: 50/352 | Loss: 0.384 | Acc: 87.59% (5606/6400)
Train Batch: 100/352 | Loss: 0.383 | Acc: 87.30% (11174/12800)
Train Batch: 150/352 | Loss: 0.385 | Acc: 87.17% (16737/19200)
Train Batch: 200/352 | Loss: 0.383 | Acc: 87.28% (22343/25600)
Train Batch: 250/352 | Loss: 0.384 | Acc: 87.25% (27921/32000)
Train Batch: 300/352 | Loss: 0.388 | Acc: 87.15% (33465/38400)
Train Batch: 350/352 | Loss: 0.388 | Acc: 87.12% (39030/44800)
Train Batch: 352/352 | Loss: 0.388 | Acc: 87.13% (39208/45000)
Validation | Loss: 0.336 | Acc: 88.28% (4414/5000)

Epoch 84/200
Train Batch: 50/352 | Loss: 0.367 | Acc: 87.69% (5612/6400)
Train Batch: 100/352 | Loss: 0.371 | Acc: 87.52% (11202/12800)
Train Batch: 150/352 | Loss: 0.371 | Acc: 87.48% (16797/19200)
Train Batch: 200/352 | Loss: 0.378 | Acc: 87.36% (22364/25600)
Train Batch: 250/352 | Loss: 0.383 | Acc: 87.22% (27909/32000)
Train Batch: 300/352 | Loss: 0.386 | Acc: 87.08% (33440/38400)
Train Batch: 350/352 | Loss: 0.385 | Acc: 87.12% (39032/44800)
Train Batch: 352/352 | Loss: 0.385 | Acc: 87.13% (39208/45000)
Validation | Loss: 0.315 | Acc: 88.78% (4439/5000)

Epoch 85/200
Train Batch: 50/352 | Loss: 0.395 | Acc: 86.73% (5551/6400)
Train Batch: 100/352 | Loss: 0.384 | Acc: 87.20% (11161/12800)
Train Batch: 150/352 | Loss: 0.388 | Acc: 87.03% (16710/19200)
Train Batch: 200/352 | Loss: 0.388 | Acc: 87.04% (22282/25600)
Train Batch: 250/352 | Loss: 0.390 | Acc: 86.93% (27817/32000)
Train Batch: 300/352 | Loss: 0.388 | Acc: 87.01% (33410/38400)
Train Batch: 350/352 | Loss: 0.388 | Acc: 87.01% (38980/44800)
Train Batch: 352/352 | Loss: 0.389 | Acc: 87.00% (39148/45000)
Validation | Loss: 0.374 | Acc: 87.54% (4377/5000)

Epoch 86/200
Train Batch: 50/352 | Loss: 0.360 | Acc: 87.80% (5619/6400)
Train Batch: 100/352 | Loss: 0.371 | Acc: 87.47% (11196/12800)
Train Batch: 150/352 | Loss: 0.376 | Acc: 87.33% (16767/19200)
Train Batch: 200/352 | Loss: 0.381 | Acc: 87.23% (22331/25600)
Train Batch: 250/352 | Loss: 0.381 | Acc: 87.14% (27885/32000)
Train Batch: 300/352 | Loss: 0.383 | Acc: 87.08% (33437/38400)
Train Batch: 350/352 | Loss: 0.383 | Acc: 87.16% (39046/44800)
Train Batch: 352/352 | Loss: 0.383 | Acc: 87.15% (39219/45000)
Validation | Loss: 0.351 | Acc: 88.30% (4415/5000)

Epoch 87/200
Train Batch: 50/352 | Loss: 0.368 | Acc: 87.94% (5628/6400)
Train Batch: 100/352 | Loss: 0.372 | Acc: 87.59% (11212/12800)
Train Batch: 150/352 | Loss: 0.370 | Acc: 87.58% (16816/19200)
Train Batch: 200/352 | Loss: 0.372 | Acc: 87.56% (22415/25600)
Train Batch: 250/352 | Loss: 0.375 | Acc: 87.45% (27983/32000)
Train Batch: 300/352 | Loss: 0.379 | Acc: 87.29% (33518/38400)
Train Batch: 350/352 | Loss: 0.381 | Acc: 87.19% (39060/44800)
Train Batch: 352/352 | Loss: 0.381 | Acc: 87.17% (39227/45000)
Validation | Loss: 0.330 | Acc: 88.08% (4404/5000)

Epoch 88/200
Train Batch: 50/352 | Loss: 0.367 | Acc: 88.00% (5632/6400)
Train Batch: 100/352 | Loss: 0.359 | Acc: 87.95% (11258/12800)
Train Batch: 150/352 | Loss: 0.359 | Acc: 87.97% (16891/19200)
Train Batch: 200/352 | Loss: 0.363 | Acc: 87.80% (22477/25600)
Train Batch: 250/352 | Loss: 0.365 | Acc: 87.72% (28071/32000)
Train Batch: 300/352 | Loss: 0.365 | Acc: 87.72% (33685/38400)
Train Batch: 350/352 | Loss: 0.369 | Acc: 87.61% (39250/44800)
Train Batch: 352/352 | Loss: 0.369 | Acc: 87.60% (39420/45000)
Validation | Loss: 0.308 | Acc: 89.10% (4455/5000)

Epoch 89/200
Train Batch: 50/352 | Loss: 0.357 | Acc: 87.92% (5627/6400)
Train Batch: 100/352 | Loss: 0.348 | Acc: 88.24% (11295/12800)
Train Batch: 150/352 | Loss: 0.352 | Acc: 88.26% (16946/19200)
Train Batch: 200/352 | Loss: 0.358 | Acc: 87.91% (22506/25600)
Train Batch: 250/352 | Loss: 0.365 | Acc: 87.71% (28067/32000)
Train Batch: 300/352 | Loss: 0.363 | Acc: 87.78% (33706/38400)
Train Batch: 350/352 | Loss: 0.365 | Acc: 87.71% (39296/44800)
Train Batch: 352/352 | Loss: 0.366 | Acc: 87.70% (39466/45000)
Validation | Loss: 0.387 | Acc: 86.98% (4349/5000)

Epoch 90/200
Train Batch: 50/352 | Loss: 0.353 | Acc: 88.36% (5655/6400)
Train Batch: 100/352 | Loss: 0.360 | Acc: 88.25% (11296/12800)
Train Batch: 150/352 | Loss: 0.358 | Acc: 88.19% (16933/19200)
Train Batch: 200/352 | Loss: 0.363 | Acc: 87.96% (22517/25600)
Train Batch: 250/352 | Loss: 0.366 | Acc: 87.89% (28124/32000)
Train Batch: 300/352 | Loss: 0.365 | Acc: 87.87% (33742/38400)
Train Batch: 350/352 | Loss: 0.365 | Acc: 87.90% (39380/44800)
Train Batch: 352/352 | Loss: 0.365 | Acc: 87.90% (39557/45000)
Validation | Loss: 0.381 | Acc: 87.12% (4356/5000)

Epoch 91/200
Train Batch: 50/352 | Loss: 0.358 | Acc: 88.14% (5641/6400)
Train Batch: 100/352 | Loss: 0.359 | Acc: 87.91% (11252/12800)
Train Batch: 150/352 | Loss: 0.366 | Acc: 87.69% (16836/19200)
Train Batch: 200/352 | Loss: 0.365 | Acc: 87.61% (22428/25600)
Train Batch: 250/352 | Loss: 0.366 | Acc: 87.63% (28041/32000)
Train Batch: 300/352 | Loss: 0.366 | Acc: 87.64% (33653/38400)
Train Batch: 350/352 | Loss: 0.369 | Acc: 87.54% (39216/44800)
Train Batch: 352/352 | Loss: 0.370 | Acc: 87.53% (39390/45000)
Validation | Loss: 0.297 | Acc: 89.74% (4487/5000)
New best model with accuracy: 0.8974

Epoch 92/200
Train Batch: 50/352 | Loss: 0.347 | Acc: 88.41% (5658/6400)
Train Batch: 100/352 | Loss: 0.351 | Acc: 88.20% (11290/12800)
Train Batch: 150/352 | Loss: 0.360 | Acc: 87.88% (16872/19200)
Train Batch: 200/352 | Loss: 0.356 | Acc: 87.96% (22517/25600)
Train Batch: 250/352 | Loss: 0.359 | Acc: 87.91% (28131/32000)
Train Batch: 300/352 | Loss: 0.359 | Acc: 87.93% (33766/38400)
Train Batch: 350/352 | Loss: 0.359 | Acc: 87.97% (39410/44800)
Train Batch: 352/352 | Loss: 0.359 | Acc: 87.96% (39584/45000)
Validation | Loss: 0.347 | Acc: 88.70% (4435/5000)

Epoch 93/200
Train Batch: 50/352 | Loss: 0.355 | Acc: 88.19% (5644/6400)
Train Batch: 100/352 | Loss: 0.351 | Acc: 88.26% (11297/12800)
Train Batch: 150/352 | Loss: 0.351 | Acc: 88.11% (16918/19200)
Train Batch: 200/352 | Loss: 0.354 | Acc: 88.11% (22556/25600)
Train Batch: 250/352 | Loss: 0.353 | Acc: 88.21% (28226/32000)
Train Batch: 300/352 | Loss: 0.355 | Acc: 88.05% (33813/38400)
Train Batch: 350/352 | Loss: 0.357 | Acc: 87.94% (39396/44800)
Train Batch: 352/352 | Loss: 0.357 | Acc: 87.94% (39575/45000)
Validation | Loss: 0.324 | Acc: 88.60% (4430/5000)

Epoch 94/200
Train Batch: 50/352 | Loss: 0.350 | Acc: 88.58% (5669/6400)
Train Batch: 100/352 | Loss: 0.341 | Acc: 88.76% (11361/12800)
Train Batch: 150/352 | Loss: 0.339 | Acc: 88.72% (17035/19200)
Train Batch: 200/352 | Loss: 0.343 | Acc: 88.61% (22684/25600)
Train Batch: 250/352 | Loss: 0.348 | Acc: 88.42% (28296/32000)
Train Batch: 300/352 | Loss: 0.349 | Acc: 88.45% (33964/38400)
Train Batch: 350/352 | Loss: 0.352 | Acc: 88.28% (39548/44800)
Train Batch: 352/352 | Loss: 0.353 | Acc: 88.28% (39725/45000)
Validation | Loss: 0.299 | Acc: 89.64% (4482/5000)

Epoch 95/200
Train Batch: 50/352 | Loss: 0.339 | Acc: 88.34% (5654/6400)
Train Batch: 100/352 | Loss: 0.329 | Acc: 88.79% (11365/12800)
Train Batch: 150/352 | Loss: 0.338 | Acc: 88.56% (17004/19200)
Train Batch: 200/352 | Loss: 0.342 | Acc: 88.40% (22630/25600)
Train Batch: 250/352 | Loss: 0.349 | Acc: 88.16% (28210/32000)
Train Batch: 300/352 | Loss: 0.353 | Acc: 88.10% (33831/38400)
Train Batch: 350/352 | Loss: 0.353 | Acc: 88.05% (39448/44800)
Train Batch: 352/352 | Loss: 0.353 | Acc: 88.05% (39624/45000)
Validation | Loss: 0.321 | Acc: 88.28% (4414/5000)

Epoch 96/200
Train Batch: 50/352 | Loss: 0.340 | Acc: 88.53% (5666/6400)
Train Batch: 100/352 | Loss: 0.342 | Acc: 88.50% (11328/12800)
Train Batch: 150/352 | Loss: 0.343 | Acc: 88.57% (17005/19200)
Train Batch: 200/352 | Loss: 0.343 | Acc: 88.45% (22644/25600)
Train Batch: 250/352 | Loss: 0.345 | Acc: 88.38% (28281/32000)
Train Batch: 300/352 | Loss: 0.348 | Acc: 88.33% (33917/38400)
Train Batch: 350/352 | Loss: 0.351 | Acc: 88.23% (39529/44800)
Train Batch: 352/352 | Loss: 0.351 | Acc: 88.22% (39701/45000)
Validation | Loss: 0.306 | Acc: 89.48% (4474/5000)

Epoch 97/200
Train Batch: 50/352 | Loss: 0.337 | Acc: 88.72% (5678/6400)
Train Batch: 100/352 | Loss: 0.342 | Acc: 88.38% (11312/12800)
Train Batch: 150/352 | Loss: 0.346 | Acc: 88.26% (16946/19200)
Train Batch: 200/352 | Loss: 0.347 | Acc: 88.19% (22576/25600)
Train Batch: 250/352 | Loss: 0.347 | Acc: 88.26% (28242/32000)
Train Batch: 300/352 | Loss: 0.348 | Acc: 88.28% (33899/38400)
Train Batch: 350/352 | Loss: 0.349 | Acc: 88.21% (39518/44800)
Train Batch: 352/352 | Loss: 0.348 | Acc: 88.21% (39694/45000)
Validation | Loss: 0.307 | Acc: 89.44% (4472/5000)

Epoch 98/200
Train Batch: 50/352 | Loss: 0.307 | Acc: 89.86% (5751/6400)
Train Batch: 100/352 | Loss: 0.324 | Acc: 89.12% (11407/12800)
Train Batch: 150/352 | Loss: 0.335 | Acc: 88.77% (17044/19200)
Train Batch: 200/352 | Loss: 0.338 | Acc: 88.65% (22695/25600)
Train Batch: 250/352 | Loss: 0.336 | Acc: 88.66% (28371/32000)
Train Batch: 300/352 | Loss: 0.342 | Acc: 88.54% (33999/38400)
Train Batch: 350/352 | Loss: 0.341 | Acc: 88.56% (39673/44800)
Train Batch: 352/352 | Loss: 0.341 | Acc: 88.56% (39852/45000)
Validation | Loss: 0.299 | Acc: 89.98% (4499/5000)
New best model with accuracy: 0.8998

Epoch 99/200
Train Batch: 50/352 | Loss: 0.337 | Acc: 88.75% (5680/6400)
Train Batch: 100/352 | Loss: 0.327 | Acc: 89.05% (11399/12800)
Train Batch: 150/352 | Loss: 0.332 | Acc: 88.77% (17043/19200)
Train Batch: 200/352 | Loss: 0.340 | Acc: 88.62% (22688/25600)
Train Batch: 250/352 | Loss: 0.345 | Acc: 88.47% (28310/32000)
Train Batch: 300/352 | Loss: 0.346 | Acc: 88.44% (33960/38400)
Train Batch: 350/352 | Loss: 0.348 | Acc: 88.44% (39623/44800)
Train Batch: 352/352 | Loss: 0.348 | Acc: 88.45% (39803/45000)
Validation | Loss: 0.289 | Acc: 89.66% (4483/5000)

Epoch 100/200
Train Batch: 50/352 | Loss: 0.332 | Acc: 88.42% (5659/6400)
Train Batch: 100/352 | Loss: 0.334 | Acc: 88.39% (11314/12800)
Train Batch: 150/352 | Loss: 0.329 | Acc: 88.88% (17064/19200)
Train Batch: 200/352 | Loss: 0.331 | Acc: 88.78% (22728/25600)
Train Batch: 250/352 | Loss: 0.336 | Acc: 88.64% (28364/32000)
Train Batch: 300/352 | Loss: 0.336 | Acc: 88.59% (34019/38400)
Train Batch: 350/352 | Loss: 0.336 | Acc: 88.59% (39688/44800)
Train Batch: 352/352 | Loss: 0.336 | Acc: 88.59% (39867/45000)
Validation | Loss: 0.266 | Acc: 91.02% (4551/5000)
New best model with accuracy: 0.9102

Epoch 101/200
Train Batch: 50/352 | Loss: 0.303 | Acc: 89.61% (5735/6400)
Train Batch: 100/352 | Loss: 0.321 | Acc: 89.29% (11429/12800)
Train Batch: 150/352 | Loss: 0.329 | Acc: 88.98% (17085/19200)
Train Batch: 200/352 | Loss: 0.329 | Acc: 88.92% (22764/25600)
Train Batch: 250/352 | Loss: 0.330 | Acc: 88.96% (28468/32000)
Train Batch: 300/352 | Loss: 0.332 | Acc: 88.92% (34147/38400)
Train Batch: 350/352 | Loss: 0.332 | Acc: 88.90% (39825/44800)
Train Batch: 352/352 | Loss: 0.333 | Acc: 88.88% (39998/45000)
Validation | Loss: 0.351 | Acc: 87.54% (4377/5000)

Epoch 102/200
Train Batch: 50/352 | Loss: 0.339 | Acc: 88.42% (5659/6400)
Train Batch: 100/352 | Loss: 0.341 | Acc: 88.20% (11290/12800)
Train Batch: 150/352 | Loss: 0.336 | Acc: 88.44% (16980/19200)
Train Batch: 200/352 | Loss: 0.333 | Acc: 88.60% (22682/25600)
Train Batch: 250/352 | Loss: 0.335 | Acc: 88.59% (28349/32000)
Train Batch: 300/352 | Loss: 0.334 | Acc: 88.64% (34037/38400)
Train Batch: 350/352 | Loss: 0.336 | Acc: 88.57% (39680/44800)
Train Batch: 352/352 | Loss: 0.336 | Acc: 88.58% (39862/45000)
Validation | Loss: 0.295 | Acc: 89.82% (4491/5000)

Epoch 103/200
Train Batch: 50/352 | Loss: 0.328 | Acc: 88.77% (5681/6400)
Train Batch: 100/352 | Loss: 0.320 | Acc: 89.22% (11420/12800)
Train Batch: 150/352 | Loss: 0.324 | Acc: 89.08% (17104/19200)
Train Batch: 200/352 | Loss: 0.326 | Acc: 89.00% (22784/25600)
Train Batch: 250/352 | Loss: 0.328 | Acc: 88.94% (28462/32000)
Train Batch: 300/352 | Loss: 0.328 | Acc: 88.96% (34159/38400)
Train Batch: 350/352 | Loss: 0.332 | Acc: 88.86% (39808/44800)
Train Batch: 352/352 | Loss: 0.332 | Acc: 88.86% (39988/45000)
Validation | Loss: 0.337 | Acc: 88.18% (4409/5000)

Epoch 104/200
Train Batch: 50/352 | Loss: 0.311 | Acc: 89.64% (5737/6400)
Train Batch: 100/352 | Loss: 0.323 | Acc: 89.05% (11398/12800)
Train Batch: 150/352 | Loss: 0.327 | Acc: 89.14% (17115/19200)
Train Batch: 200/352 | Loss: 0.325 | Acc: 89.21% (22837/25600)
Train Batch: 250/352 | Loss: 0.326 | Acc: 89.07% (28503/32000)
Train Batch: 300/352 | Loss: 0.326 | Acc: 89.05% (34194/38400)
Train Batch: 350/352 | Loss: 0.326 | Acc: 89.10% (39917/44800)
Train Batch: 352/352 | Loss: 0.327 | Acc: 89.10% (40096/45000)
Validation | Loss: 0.304 | Acc: 89.70% (4485/5000)

Epoch 105/200
Train Batch: 50/352 | Loss: 0.305 | Acc: 89.38% (5720/6400)
Train Batch: 100/352 | Loss: 0.305 | Acc: 89.50% (11456/12800)
Train Batch: 150/352 | Loss: 0.305 | Acc: 89.53% (17190/19200)
Train Batch: 200/352 | Loss: 0.307 | Acc: 89.52% (22918/25600)
Train Batch: 250/352 | Loss: 0.311 | Acc: 89.53% (28648/32000)
Train Batch: 300/352 | Loss: 0.315 | Acc: 89.36% (34316/38400)
Train Batch: 350/352 | Loss: 0.320 | Acc: 89.22% (39969/44800)
Train Batch: 352/352 | Loss: 0.320 | Acc: 89.23% (40152/45000)
Validation | Loss: 0.274 | Acc: 90.26% (4513/5000)

Epoch 106/200
Train Batch: 50/352 | Loss: 0.313 | Acc: 89.58% (5733/6400)
Train Batch: 100/352 | Loss: 0.309 | Acc: 89.63% (11473/12800)
Train Batch: 150/352 | Loss: 0.308 | Acc: 89.52% (17187/19200)
Train Batch: 200/352 | Loss: 0.314 | Acc: 89.35% (22874/25600)
Train Batch: 250/352 | Loss: 0.315 | Acc: 89.38% (28603/32000)
Train Batch: 300/352 | Loss: 0.315 | Acc: 89.36% (34316/38400)
Train Batch: 350/352 | Loss: 0.319 | Acc: 89.24% (39979/44800)
Train Batch: 352/352 | Loss: 0.318 | Acc: 89.25% (40164/45000)
Validation | Loss: 0.277 | Acc: 90.12% (4506/5000)

Epoch 107/200
Train Batch: 50/352 | Loss: 0.305 | Acc: 89.89% (5753/6400)
Train Batch: 100/352 | Loss: 0.293 | Acc: 90.25% (11552/12800)
Train Batch: 150/352 | Loss: 0.304 | Acc: 89.88% (17256/19200)
Train Batch: 200/352 | Loss: 0.308 | Acc: 89.72% (22968/25600)
Train Batch: 250/352 | Loss: 0.316 | Acc: 89.40% (28607/32000)
Train Batch: 300/352 | Loss: 0.318 | Acc: 89.33% (34303/38400)
Train Batch: 350/352 | Loss: 0.319 | Acc: 89.31% (40009/44800)
Train Batch: 352/352 | Loss: 0.319 | Acc: 89.31% (40191/45000)
Validation | Loss: 0.305 | Acc: 89.56% (4478/5000)

Epoch 108/200
Train Batch: 50/352 | Loss: 0.311 | Acc: 89.28% (5714/6400)
Train Batch: 100/352 | Loss: 0.312 | Acc: 89.51% (11457/12800)
Train Batch: 150/352 | Loss: 0.308 | Acc: 89.67% (17216/19200)
Train Batch: 200/352 | Loss: 0.309 | Acc: 89.79% (22985/25600)
Train Batch: 250/352 | Loss: 0.312 | Acc: 89.66% (28691/32000)
Train Batch: 300/352 | Loss: 0.315 | Acc: 89.58% (34398/38400)
Train Batch: 350/352 | Loss: 0.315 | Acc: 89.58% (40131/44800)
Train Batch: 352/352 | Loss: 0.316 | Acc: 89.58% (40309/45000)
Validation | Loss: 0.289 | Acc: 90.30% (4515/5000)

Epoch 109/200
Train Batch: 50/352 | Loss: 0.300 | Acc: 89.86% (5751/6400)
Train Batch: 100/352 | Loss: 0.306 | Acc: 89.73% (11486/12800)
Train Batch: 150/352 | Loss: 0.299 | Acc: 90.04% (17287/19200)
Train Batch: 200/352 | Loss: 0.304 | Acc: 89.91% (23018/25600)
Train Batch: 250/352 | Loss: 0.307 | Acc: 89.82% (28741/32000)
Train Batch: 300/352 | Loss: 0.307 | Acc: 89.80% (34484/38400)
Train Batch: 350/352 | Loss: 0.307 | Acc: 89.78% (40220/44800)
Train Batch: 352/352 | Loss: 0.307 | Acc: 89.78% (40403/45000)
Validation | Loss: 0.278 | Acc: 90.24% (4512/5000)

Epoch 110/200
Train Batch: 50/352 | Loss: 0.301 | Acc: 89.58% (5733/6400)
Train Batch: 100/352 | Loss: 0.302 | Acc: 89.73% (11485/12800)
Train Batch: 150/352 | Loss: 0.307 | Acc: 89.63% (17209/19200)
Train Batch: 200/352 | Loss: 0.304 | Acc: 89.72% (22968/25600)
Train Batch: 250/352 | Loss: 0.305 | Acc: 89.77% (28725/32000)
Train Batch: 300/352 | Loss: 0.308 | Acc: 89.75% (34464/38400)
Train Batch: 350/352 | Loss: 0.309 | Acc: 89.75% (40207/44800)
Train Batch: 352/352 | Loss: 0.308 | Acc: 89.76% (40391/45000)
Validation | Loss: 0.301 | Acc: 89.32% (4466/5000)

Epoch 111/200
Train Batch: 50/352 | Loss: 0.295 | Acc: 90.25% (5776/6400)
Train Batch: 100/352 | Loss: 0.296 | Acc: 89.88% (11505/12800)
Train Batch: 150/352 | Loss: 0.299 | Acc: 89.85% (17251/19200)
Train Batch: 200/352 | Loss: 0.300 | Acc: 89.73% (22972/25600)
Train Batch: 250/352 | Loss: 0.299 | Acc: 89.83% (28747/32000)
Train Batch: 300/352 | Loss: 0.300 | Acc: 89.81% (34487/38400)
Train Batch: 350/352 | Loss: 0.302 | Acc: 89.73% (40197/44800)
Train Batch: 352/352 | Loss: 0.303 | Acc: 89.72% (40375/45000)
Validation | Loss: 0.292 | Acc: 89.96% (4498/5000)

Epoch 112/200
Train Batch: 50/352 | Loss: 0.289 | Acc: 90.73% (5807/6400)
Train Batch: 100/352 | Loss: 0.292 | Acc: 90.44% (11576/12800)
Train Batch: 150/352 | Loss: 0.296 | Acc: 90.31% (17340/19200)
Train Batch: 200/352 | Loss: 0.299 | Acc: 90.13% (23073/25600)
Train Batch: 250/352 | Loss: 0.303 | Acc: 89.94% (28781/32000)
Train Batch: 300/352 | Loss: 0.303 | Acc: 89.90% (34520/38400)
Train Batch: 350/352 | Loss: 0.302 | Acc: 89.91% (40278/44800)
Train Batch: 352/352 | Loss: 0.303 | Acc: 89.90% (40457/45000)
Validation | Loss: 0.301 | Acc: 89.34% (4467/5000)

Epoch 113/200
Train Batch: 50/352 | Loss: 0.298 | Acc: 89.55% (5731/6400)
Train Batch: 100/352 | Loss: 0.295 | Acc: 89.97% (11516/12800)
Train Batch: 150/352 | Loss: 0.299 | Acc: 89.95% (17270/19200)
Train Batch: 200/352 | Loss: 0.302 | Acc: 89.93% (23021/25600)
Train Batch: 250/352 | Loss: 0.304 | Acc: 89.77% (28727/32000)
Train Batch: 300/352 | Loss: 0.303 | Acc: 89.80% (34484/38400)
Train Batch: 350/352 | Loss: 0.302 | Acc: 89.82% (40240/44800)
Train Batch: 352/352 | Loss: 0.303 | Acc: 89.82% (40417/45000)
Validation | Loss: 0.268 | Acc: 90.74% (4537/5000)

Epoch 114/200
Train Batch: 50/352 | Loss: 0.274 | Acc: 91.08% (5829/6400)
Train Batch: 100/352 | Loss: 0.277 | Acc: 90.97% (11644/12800)
Train Batch: 150/352 | Loss: 0.276 | Acc: 90.92% (17456/19200)
Train Batch: 200/352 | Loss: 0.278 | Acc: 90.66% (23208/25600)
Train Batch: 250/352 | Loss: 0.281 | Acc: 90.58% (28985/32000)
Train Batch: 300/352 | Loss: 0.281 | Acc: 90.61% (34795/38400)
Train Batch: 350/352 | Loss: 0.285 | Acc: 90.55% (40565/44800)
Train Batch: 352/352 | Loss: 0.285 | Acc: 90.54% (40744/45000)
Validation | Loss: 0.254 | Acc: 91.18% (4559/5000)
New best model with accuracy: 0.9118

Epoch 115/200
Train Batch: 50/352 | Loss: 0.267 | Acc: 91.16% (5834/6400)
Train Batch: 100/352 | Loss: 0.270 | Acc: 91.13% (11665/12800)
Train Batch: 150/352 | Loss: 0.272 | Acc: 91.10% (17491/19200)
Train Batch: 200/352 | Loss: 0.280 | Acc: 90.84% (23254/25600)
Train Batch: 250/352 | Loss: 0.284 | Acc: 90.68% (29019/32000)
Train Batch: 300/352 | Loss: 0.286 | Acc: 90.57% (34778/38400)
Train Batch: 350/352 | Loss: 0.288 | Acc: 90.54% (40562/44800)
Train Batch: 352/352 | Loss: 0.288 | Acc: 90.55% (40749/45000)
Validation | Loss: 0.262 | Acc: 90.70% (4535/5000)

Epoch 116/200
Train Batch: 50/352 | Loss: 0.274 | Acc: 90.75% (5808/6400)
Train Batch: 100/352 | Loss: 0.283 | Acc: 90.55% (11590/12800)
Train Batch: 150/352 | Loss: 0.286 | Acc: 90.33% (17344/19200)
Train Batch: 200/352 | Loss: 0.283 | Acc: 90.41% (23144/25600)
Train Batch: 250/352 | Loss: 0.283 | Acc: 90.49% (28957/32000)
Train Batch: 300/352 | Loss: 0.283 | Acc: 90.55% (34770/38400)
Train Batch: 350/352 | Loss: 0.285 | Acc: 90.45% (40521/44800)
Train Batch: 352/352 | Loss: 0.285 | Acc: 90.44% (40699/45000)
Validation | Loss: 0.267 | Acc: 90.64% (4532/5000)

Epoch 117/200
Train Batch: 50/352 | Loss: 0.274 | Acc: 90.30% (5779/6400)
Train Batch: 100/352 | Loss: 0.283 | Acc: 90.24% (11551/12800)
Train Batch: 150/352 | Loss: 0.284 | Acc: 90.30% (17338/19200)
Train Batch: 200/352 | Loss: 0.282 | Acc: 90.41% (23146/25600)
Train Batch: 250/352 | Loss: 0.281 | Acc: 90.48% (28954/32000)
Train Batch: 300/352 | Loss: 0.282 | Acc: 90.54% (34769/38400)
Train Batch: 350/352 | Loss: 0.283 | Acc: 90.54% (40564/44800)
Train Batch: 352/352 | Loss: 0.283 | Acc: 90.53% (40738/45000)
Validation | Loss: 0.255 | Acc: 91.06% (4553/5000)

Epoch 118/200
Train Batch: 50/352 | Loss: 0.272 | Acc: 90.84% (5814/6400)
Train Batch: 100/352 | Loss: 0.272 | Acc: 90.83% (11626/12800)
Train Batch: 150/352 | Loss: 0.281 | Acc: 90.49% (17374/19200)
Train Batch: 200/352 | Loss: 0.278 | Acc: 90.66% (23210/25600)
Train Batch: 250/352 | Loss: 0.281 | Acc: 90.57% (28982/32000)
Train Batch: 300/352 | Loss: 0.280 | Acc: 90.54% (34766/38400)
Train Batch: 350/352 | Loss: 0.279 | Acc: 90.56% (40572/44800)
Train Batch: 352/352 | Loss: 0.278 | Acc: 90.57% (40755/45000)
Validation | Loss: 0.292 | Acc: 89.62% (4481/5000)

Epoch 119/200
Train Batch: 50/352 | Loss: 0.271 | Acc: 90.77% (5809/6400)
Train Batch: 100/352 | Loss: 0.275 | Acc: 90.62% (11600/12800)
Train Batch: 150/352 | Loss: 0.274 | Acc: 90.66% (17406/19200)
Train Batch: 200/352 | Loss: 0.276 | Acc: 90.58% (23189/25600)
Train Batch: 250/352 | Loss: 0.276 | Acc: 90.61% (28994/32000)
Train Batch: 300/352 | Loss: 0.276 | Acc: 90.65% (34808/38400)
Train Batch: 350/352 | Loss: 0.277 | Acc: 90.59% (40583/44800)
Train Batch: 352/352 | Loss: 0.277 | Acc: 90.57% (40758/45000)
Validation | Loss: 0.272 | Acc: 90.78% (4539/5000)

Epoch 120/200
Train Batch: 50/352 | Loss: 0.262 | Acc: 91.39% (5849/6400)
Train Batch: 100/352 | Loss: 0.268 | Acc: 91.02% (11651/12800)
Train Batch: 150/352 | Loss: 0.278 | Acc: 90.56% (17387/19200)
Train Batch: 200/352 | Loss: 0.279 | Acc: 90.55% (23181/25600)
Train Batch: 250/352 | Loss: 0.278 | Acc: 90.62% (28998/32000)
Train Batch: 300/352 | Loss: 0.277 | Acc: 90.66% (34814/38400)
Train Batch: 350/352 | Loss: 0.278 | Acc: 90.56% (40571/44800)
Train Batch: 352/352 | Loss: 0.278 | Acc: 90.56% (40750/45000)
Validation | Loss: 0.256 | Acc: 91.12% (4556/5000)

Epoch 121/200
Train Batch: 50/352 | Loss: 0.259 | Acc: 91.09% (5830/6400)
Train Batch: 100/352 | Loss: 0.262 | Acc: 91.23% (11678/12800)
Train Batch: 150/352 | Loss: 0.265 | Acc: 91.20% (17511/19200)
Train Batch: 200/352 | Loss: 0.264 | Acc: 91.18% (23341/25600)
Train Batch: 250/352 | Loss: 0.264 | Acc: 91.11% (29154/32000)
Train Batch: 300/352 | Loss: 0.268 | Acc: 90.97% (34931/38400)
Train Batch: 350/352 | Loss: 0.271 | Acc: 90.88% (40716/44800)
Train Batch: 352/352 | Loss: 0.271 | Acc: 90.88% (40896/45000)
Validation | Loss: 0.264 | Acc: 90.62% (4531/5000)

Epoch 122/200
Train Batch: 50/352 | Loss: 0.279 | Acc: 90.48% (5791/6400)
Train Batch: 100/352 | Loss: 0.266 | Acc: 90.98% (11645/12800)
Train Batch: 150/352 | Loss: 0.263 | Acc: 91.09% (17490/19200)
Train Batch: 200/352 | Loss: 0.265 | Acc: 91.02% (23301/25600)
Train Batch: 250/352 | Loss: 0.264 | Acc: 91.10% (29151/32000)
Train Batch: 300/352 | Loss: 0.267 | Acc: 90.97% (34932/38400)
Train Batch: 350/352 | Loss: 0.270 | Acc: 90.92% (40731/44800)
Train Batch: 352/352 | Loss: 0.270 | Acc: 90.90% (40906/45000)
Validation | Loss: 0.275 | Acc: 90.46% (4523/5000)

Epoch 123/200
Train Batch: 50/352 | Loss: 0.250 | Acc: 91.45% (5853/6400)
Train Batch: 100/352 | Loss: 0.249 | Acc: 91.55% (11718/12800)
Train Batch: 150/352 | Loss: 0.251 | Acc: 91.51% (17570/19200)
Train Batch: 200/352 | Loss: 0.254 | Acc: 91.44% (23409/25600)
Train Batch: 250/352 | Loss: 0.254 | Acc: 91.42% (29253/32000)
Train Batch: 300/352 | Loss: 0.259 | Acc: 91.31% (35063/38400)
Train Batch: 350/352 | Loss: 0.260 | Acc: 91.27% (40891/44800)
Train Batch: 352/352 | Loss: 0.261 | Acc: 91.27% (41070/45000)
Validation | Loss: 0.252 | Acc: 91.54% (4577/5000)
New best model with accuracy: 0.9154

Epoch 124/200
Train Batch: 50/352 | Loss: 0.246 | Acc: 91.59% (5862/6400)
Train Batch: 100/352 | Loss: 0.252 | Acc: 91.38% (11697/12800)
Train Batch: 150/352 | Loss: 0.254 | Acc: 91.41% (17550/19200)
Train Batch: 200/352 | Loss: 0.253 | Acc: 91.53% (23431/25600)
Train Batch: 250/352 | Loss: 0.254 | Acc: 91.47% (29269/32000)
Train Batch: 300/352 | Loss: 0.255 | Acc: 91.49% (35131/38400)
Train Batch: 350/352 | Loss: 0.257 | Acc: 91.36% (40931/44800)
Train Batch: 352/352 | Loss: 0.257 | Acc: 91.37% (41115/45000)
Validation | Loss: 0.237 | Acc: 91.98% (4599/5000)
New best model with accuracy: 0.9198

Epoch 125/200
Train Batch: 50/352 | Loss: 0.232 | Acc: 92.19% (5900/6400)
Train Batch: 100/352 | Loss: 0.239 | Acc: 91.87% (11759/12800)
Train Batch: 150/352 | Loss: 0.243 | Acc: 91.91% (17646/19200)
Train Batch: 200/352 | Loss: 0.242 | Acc: 91.83% (23509/25600)
Train Batch: 250/352 | Loss: 0.242 | Acc: 91.83% (29387/32000)
Train Batch: 300/352 | Loss: 0.246 | Acc: 91.67% (35202/38400)
Train Batch: 350/352 | Loss: 0.250 | Acc: 91.57% (41024/44800)
Train Batch: 352/352 | Loss: 0.250 | Acc: 91.56% (41203/45000)
Validation | Loss: 0.245 | Acc: 91.56% (4578/5000)

Epoch 126/200
Train Batch: 50/352 | Loss: 0.236 | Acc: 92.00% (5888/6400)
Train Batch: 100/352 | Loss: 0.236 | Acc: 91.95% (11769/12800)
Train Batch: 150/352 | Loss: 0.235 | Acc: 92.01% (17665/19200)
Train Batch: 200/352 | Loss: 0.238 | Acc: 91.95% (23538/25600)
Train Batch: 250/352 | Loss: 0.243 | Acc: 91.77% (29365/32000)
Train Batch: 300/352 | Loss: 0.247 | Acc: 91.65% (35195/38400)
Train Batch: 350/352 | Loss: 0.250 | Acc: 91.58% (41028/44800)
Train Batch: 352/352 | Loss: 0.250 | Acc: 91.58% (41209/45000)
Validation | Loss: 0.241 | Acc: 91.72% (4586/5000)

Epoch 127/200
Train Batch: 50/352 | Loss: 0.235 | Acc: 92.31% (5908/6400)
Train Batch: 100/352 | Loss: 0.244 | Acc: 91.80% (11750/12800)
Train Batch: 150/352 | Loss: 0.241 | Acc: 91.91% (17646/19200)
Train Batch: 200/352 | Loss: 0.240 | Acc: 91.91% (23529/25600)
Train Batch: 250/352 | Loss: 0.243 | Acc: 91.86% (29395/32000)
Train Batch: 300/352 | Loss: 0.246 | Acc: 91.75% (35231/38400)
Train Batch: 350/352 | Loss: 0.252 | Acc: 91.54% (41009/44800)
Train Batch: 352/352 | Loss: 0.251 | Acc: 91.55% (41198/45000)
Validation | Loss: 0.281 | Acc: 90.30% (4515/5000)

Epoch 128/200
Train Batch: 50/352 | Loss: 0.241 | Acc: 91.94% (5884/6400)
Train Batch: 100/352 | Loss: 0.233 | Acc: 92.16% (11797/12800)
Train Batch: 150/352 | Loss: 0.233 | Acc: 92.19% (17701/19200)
Train Batch: 200/352 | Loss: 0.237 | Acc: 92.11% (23580/25600)
Train Batch: 250/352 | Loss: 0.236 | Acc: 92.11% (29474/32000)
Train Batch: 300/352 | Loss: 0.237 | Acc: 92.06% (35350/38400)
Train Batch: 350/352 | Loss: 0.242 | Acc: 91.89% (41166/44800)
Train Batch: 352/352 | Loss: 0.242 | Acc: 91.88% (41346/45000)
Validation | Loss: 0.252 | Acc: 91.92% (4596/5000)

Epoch 129/200
Train Batch: 50/352 | Loss: 0.239 | Acc: 91.75% (5872/6400)
Train Batch: 100/352 | Loss: 0.238 | Acc: 91.84% (11756/12800)
Train Batch: 150/352 | Loss: 0.241 | Acc: 91.68% (17603/19200)
Train Batch: 200/352 | Loss: 0.239 | Acc: 91.78% (23495/25600)
Train Batch: 250/352 | Loss: 0.240 | Acc: 91.76% (29362/32000)
Train Batch: 300/352 | Loss: 0.240 | Acc: 91.82% (35260/38400)
Train Batch: 350/352 | Loss: 0.240 | Acc: 91.83% (41138/44800)
Train Batch: 352/352 | Loss: 0.241 | Acc: 91.82% (41320/45000)
Validation | Loss: 0.250 | Acc: 91.64% (4582/5000)

Epoch 130/200
Train Batch: 50/352 | Loss: 0.214 | Acc: 92.44% (5916/6400)
Train Batch: 100/352 | Loss: 0.223 | Acc: 92.31% (11816/12800)
Train Batch: 150/352 | Loss: 0.226 | Acc: 92.18% (17698/19200)
Train Batch: 200/352 | Loss: 0.225 | Acc: 92.23% (23610/25600)
Train Batch: 250/352 | Loss: 0.228 | Acc: 92.11% (29476/32000)
Train Batch: 300/352 | Loss: 0.228 | Acc: 92.12% (35373/38400)
Train Batch: 350/352 | Loss: 0.233 | Acc: 91.99% (41211/44800)
Train Batch: 352/352 | Loss: 0.233 | Acc: 92.00% (41398/45000)
Validation | Loss: 0.247 | Acc: 91.52% (4576/5000)

Epoch 131/200
Train Batch: 50/352 | Loss: 0.217 | Acc: 92.66% (5930/6400)
Train Batch: 100/352 | Loss: 0.226 | Acc: 92.40% (11827/12800)
Train Batch: 150/352 | Loss: 0.234 | Acc: 92.06% (17676/19200)
Train Batch: 200/352 | Loss: 0.234 | Acc: 92.04% (23562/25600)
Train Batch: 250/352 | Loss: 0.233 | Acc: 92.10% (29472/32000)
Train Batch: 300/352 | Loss: 0.235 | Acc: 92.04% (35342/38400)
Train Batch: 350/352 | Loss: 0.235 | Acc: 92.04% (41236/44800)
Train Batch: 352/352 | Loss: 0.235 | Acc: 92.04% (41420/45000)
Validation | Loss: 0.252 | Acc: 91.38% (4569/5000)

Epoch 132/200
Train Batch: 50/352 | Loss: 0.218 | Acc: 92.67% (5931/6400)
Train Batch: 100/352 | Loss: 0.222 | Acc: 92.70% (11866/12800)
Train Batch: 150/352 | Loss: 0.222 | Acc: 92.67% (17793/19200)
Train Batch: 200/352 | Loss: 0.222 | Acc: 92.59% (23704/25600)
Train Batch: 250/352 | Loss: 0.221 | Acc: 92.59% (29630/32000)
Train Batch: 300/352 | Loss: 0.222 | Acc: 92.61% (35561/38400)
Train Batch: 350/352 | Loss: 0.225 | Acc: 92.49% (41434/44800)
Train Batch: 352/352 | Loss: 0.225 | Acc: 92.50% (41623/45000)
Validation | Loss: 0.273 | Acc: 90.98% (4549/5000)

Epoch 133/200
Train Batch: 50/352 | Loss: 0.208 | Acc: 92.91% (5946/6400)
Train Batch: 100/352 | Loss: 0.209 | Acc: 92.84% (11884/12800)
Train Batch: 150/352 | Loss: 0.211 | Acc: 92.86% (17829/19200)
Train Batch: 200/352 | Loss: 0.213 | Acc: 92.88% (23778/25600)
Train Batch: 250/352 | Loss: 0.219 | Acc: 92.70% (29665/32000)
Train Batch: 300/352 | Loss: 0.220 | Acc: 92.70% (35598/38400)
Train Batch: 350/352 | Loss: 0.222 | Acc: 92.62% (41496/44800)
Train Batch: 352/352 | Loss: 0.222 | Acc: 92.62% (41681/45000)
Validation | Loss: 0.234 | Acc: 92.14% (4607/5000)
New best model with accuracy: 0.9214

Epoch 134/200
Train Batch: 50/352 | Loss: 0.209 | Acc: 93.14% (5961/6400)
Train Batch: 100/352 | Loss: 0.209 | Acc: 92.96% (11899/12800)
Train Batch: 150/352 | Loss: 0.216 | Acc: 92.73% (17804/19200)
Train Batch: 200/352 | Loss: 0.215 | Acc: 92.79% (23755/25600)
Train Batch: 250/352 | Loss: 0.217 | Acc: 92.69% (29660/32000)
Train Batch: 300/352 | Loss: 0.222 | Acc: 92.55% (35540/38400)
Train Batch: 350/352 | Loss: 0.222 | Acc: 92.52% (41449/44800)
Train Batch: 352/352 | Loss: 0.222 | Acc: 92.52% (41632/45000)
Validation | Loss: 0.211 | Acc: 92.30% (4615/5000)
New best model with accuracy: 0.9230

Epoch 135/200
Train Batch: 50/352 | Loss: 0.207 | Acc: 92.89% (5945/6400)
Train Batch: 100/352 | Loss: 0.210 | Acc: 92.91% (11892/12800)
Train Batch: 150/352 | Loss: 0.212 | Acc: 92.70% (17799/19200)
Train Batch: 200/352 | Loss: 0.216 | Acc: 92.58% (23701/25600)
Train Batch: 250/352 | Loss: 0.216 | Acc: 92.58% (29624/32000)
Train Batch: 300/352 | Loss: 0.218 | Acc: 92.55% (35539/38400)
Train Batch: 350/352 | Loss: 0.220 | Acc: 92.52% (41449/44800)
Train Batch: 352/352 | Loss: 0.220 | Acc: 92.53% (41637/45000)
Validation | Loss: 0.215 | Acc: 92.56% (4628/5000)
New best model with accuracy: 0.9256

Epoch 136/200
Train Batch: 50/352 | Loss: 0.212 | Acc: 92.97% (5950/6400)
Train Batch: 100/352 | Loss: 0.202 | Acc: 93.14% (11922/12800)
Train Batch: 150/352 | Loss: 0.196 | Acc: 93.40% (17932/19200)
Train Batch: 200/352 | Loss: 0.197 | Acc: 93.39% (23909/25600)
Train Batch: 250/352 | Loss: 0.202 | Acc: 93.24% (29838/32000)
Train Batch: 300/352 | Loss: 0.207 | Acc: 93.03% (35725/38400)
Train Batch: 350/352 | Loss: 0.205 | Acc: 93.11% (41713/44800)
Train Batch: 352/352 | Loss: 0.206 | Acc: 93.10% (41894/45000)
Validation | Loss: 0.229 | Acc: 92.30% (4615/5000)

Epoch 137/200
Train Batch: 50/352 | Loss: 0.196 | Acc: 93.38% (5976/6400)
Train Batch: 100/352 | Loss: 0.195 | Acc: 93.54% (11973/12800)
Train Batch: 150/352 | Loss: 0.196 | Acc: 93.55% (17962/19200)
Train Batch: 200/352 | Loss: 0.197 | Acc: 93.53% (23944/25600)
Train Batch: 250/352 | Loss: 0.201 | Acc: 93.41% (29891/32000)
Train Batch: 300/352 | Loss: 0.202 | Acc: 93.32% (35834/38400)
Train Batch: 350/352 | Loss: 0.205 | Acc: 93.24% (41771/44800)
Train Batch: 352/352 | Loss: 0.205 | Acc: 93.24% (41956/45000)
Validation | Loss: 0.242 | Acc: 91.80% (4590/5000)

Epoch 138/200
Train Batch: 50/352 | Loss: 0.203 | Acc: 93.14% (5961/6400)
Train Batch: 100/352 | Loss: 0.200 | Acc: 93.24% (11935/12800)
Train Batch: 150/352 | Loss: 0.202 | Acc: 93.16% (17887/19200)
Train Batch: 200/352 | Loss: 0.203 | Acc: 93.14% (23844/25600)
Train Batch: 250/352 | Loss: 0.204 | Acc: 93.08% (29785/32000)
Train Batch: 300/352 | Loss: 0.204 | Acc: 93.07% (35738/38400)
Train Batch: 350/352 | Loss: 0.203 | Acc: 93.12% (41719/44800)
Train Batch: 352/352 | Loss: 0.203 | Acc: 93.14% (41912/45000)
Validation | Loss: 0.231 | Acc: 92.14% (4607/5000)

Epoch 139/200
Train Batch: 50/352 | Loss: 0.202 | Acc: 93.12% (5960/6400)
Train Batch: 100/352 | Loss: 0.190 | Acc: 93.48% (11966/12800)
Train Batch: 150/352 | Loss: 0.192 | Acc: 93.51% (17954/19200)
Train Batch: 200/352 | Loss: 0.193 | Acc: 93.54% (23946/25600)
Train Batch: 250/352 | Loss: 0.196 | Acc: 93.42% (29894/32000)
Train Batch: 300/352 | Loss: 0.199 | Acc: 93.34% (35841/38400)
Train Batch: 350/352 | Loss: 0.201 | Acc: 93.27% (41784/44800)
Train Batch: 352/352 | Loss: 0.200 | Acc: 93.28% (41974/45000)
Validation | Loss: 0.242 | Acc: 92.08% (4604/5000)

Epoch 140/200
Train Batch: 50/352 | Loss: 0.188 | Acc: 93.66% (5994/6400)
Train Batch: 100/352 | Loss: 0.187 | Acc: 93.69% (11992/12800)
Train Batch: 150/352 | Loss: 0.186 | Acc: 93.73% (17996/19200)
Train Batch: 200/352 | Loss: 0.189 | Acc: 93.66% (23977/25600)
Train Batch: 250/352 | Loss: 0.191 | Acc: 93.61% (29955/32000)
Train Batch: 300/352 | Loss: 0.193 | Acc: 93.48% (35895/38400)
Train Batch: 350/352 | Loss: 0.195 | Acc: 93.41% (41847/44800)
Train Batch: 352/352 | Loss: 0.195 | Acc: 93.41% (42036/45000)
Validation | Loss: 0.217 | Acc: 92.48% (4624/5000)

Epoch 141/200
Train Batch: 50/352 | Loss: 0.181 | Acc: 93.84% (6006/6400)
Train Batch: 100/352 | Loss: 0.181 | Acc: 93.94% (12024/12800)
Train Batch: 150/352 | Loss: 0.183 | Acc: 93.90% (18029/19200)
Train Batch: 200/352 | Loss: 0.184 | Acc: 93.81% (24015/25600)
Train Batch: 250/352 | Loss: 0.185 | Acc: 93.80% (30017/32000)
Train Batch: 300/352 | Loss: 0.187 | Acc: 93.75% (35999/38400)
Train Batch: 350/352 | Loss: 0.188 | Acc: 93.73% (41991/44800)
Train Batch: 352/352 | Loss: 0.187 | Acc: 93.74% (42183/45000)
Validation | Loss: 0.217 | Acc: 92.46% (4623/5000)

Epoch 142/200
Train Batch: 50/352 | Loss: 0.177 | Acc: 94.38% (6040/6400)
Train Batch: 100/352 | Loss: 0.181 | Acc: 94.16% (12053/12800)
Train Batch: 150/352 | Loss: 0.182 | Acc: 94.12% (18071/19200)
Train Batch: 200/352 | Loss: 0.186 | Acc: 93.91% (24041/25600)
Train Batch: 250/352 | Loss: 0.187 | Acc: 93.90% (30048/32000)
Train Batch: 300/352 | Loss: 0.189 | Acc: 93.85% (36037/38400)
Train Batch: 350/352 | Loss: 0.190 | Acc: 93.79% (42017/44800)
Train Batch: 352/352 | Loss: 0.190 | Acc: 93.79% (42205/45000)
Validation | Loss: 0.226 | Acc: 92.18% (4609/5000)

Epoch 143/200
Train Batch: 50/352 | Loss: 0.187 | Acc: 93.44% (5980/6400)
Train Batch: 100/352 | Loss: 0.185 | Acc: 93.76% (12001/12800)
Train Batch: 150/352 | Loss: 0.188 | Acc: 93.57% (17966/19200)
Train Batch: 200/352 | Loss: 0.185 | Acc: 93.75% (24000/25600)
Train Batch: 250/352 | Loss: 0.184 | Acc: 93.77% (30007/32000)
Train Batch: 300/352 | Loss: 0.185 | Acc: 93.78% (36011/38400)
Train Batch: 350/352 | Loss: 0.185 | Acc: 93.78% (42013/44800)
Train Batch: 352/352 | Loss: 0.185 | Acc: 93.79% (42207/45000)
Validation | Loss: 0.227 | Acc: 92.70% (4635/5000)
New best model with accuracy: 0.9270

Epoch 144/200
Train Batch: 50/352 | Loss: 0.169 | Acc: 94.56% (6052/6400)
Train Batch: 100/352 | Loss: 0.170 | Acc: 94.52% (12098/12800)
Train Batch: 150/352 | Loss: 0.176 | Acc: 94.27% (18099/19200)
Train Batch: 200/352 | Loss: 0.176 | Acc: 94.15% (24102/25600)
Train Batch: 250/352 | Loss: 0.176 | Acc: 94.14% (30125/32000)
Train Batch: 300/352 | Loss: 0.176 | Acc: 94.09% (36129/38400)
Train Batch: 350/352 | Loss: 0.178 | Acc: 94.01% (42116/44800)
Train Batch: 352/352 | Loss: 0.179 | Acc: 94.00% (42300/45000)
Validation | Loss: 0.204 | Acc: 93.32% (4666/5000)
New best model with accuracy: 0.9332

Epoch 145/200
Train Batch: 50/352 | Loss: 0.166 | Acc: 94.48% (6047/6400)
Train Batch: 100/352 | Loss: 0.167 | Acc: 94.52% (12099/12800)
Train Batch: 150/352 | Loss: 0.168 | Acc: 94.55% (18154/19200)
Train Batch: 200/352 | Loss: 0.167 | Acc: 94.52% (24198/25600)
Train Batch: 250/352 | Loss: 0.170 | Acc: 94.42% (30216/32000)
Train Batch: 300/352 | Loss: 0.171 | Acc: 94.36% (36236/38400)
Train Batch: 350/352 | Loss: 0.172 | Acc: 94.26% (42229/44800)
Train Batch: 352/352 | Loss: 0.172 | Acc: 94.25% (42413/45000)
Validation | Loss: 0.220 | Acc: 92.32% (4616/5000)

Epoch 146/200
Train Batch: 50/352 | Loss: 0.170 | Acc: 94.19% (6028/6400)
Train Batch: 100/352 | Loss: 0.162 | Acc: 94.55% (12102/12800)
Train Batch: 150/352 | Loss: 0.166 | Acc: 94.41% (18126/19200)
Train Batch: 200/352 | Loss: 0.165 | Acc: 94.43% (24174/25600)
Train Batch: 250/352 | Loss: 0.165 | Acc: 94.46% (30226/32000)
Train Batch: 300/352 | Loss: 0.166 | Acc: 94.38% (36242/38400)
Train Batch: 350/352 | Loss: 0.167 | Acc: 94.36% (42274/44800)
Train Batch: 352/352 | Loss: 0.168 | Acc: 94.35% (42458/45000)
Validation | Loss: 0.228 | Acc: 92.52% (4626/5000)

Epoch 147/200
Train Batch: 50/352 | Loss: 0.159 | Acc: 94.58% (6053/6400)
Train Batch: 100/352 | Loss: 0.166 | Acc: 94.48% (12094/12800)
Train Batch: 150/352 | Loss: 0.164 | Acc: 94.61% (18166/19200)
Train Batch: 200/352 | Loss: 0.166 | Acc: 94.54% (24203/25600)
Train Batch: 250/352 | Loss: 0.164 | Acc: 94.61% (30276/32000)
Train Batch: 300/352 | Loss: 0.164 | Acc: 94.57% (36315/38400)
Train Batch: 350/352 | Loss: 0.164 | Acc: 94.56% (42362/44800)
Train Batch: 352/352 | Loss: 0.164 | Acc: 94.56% (42550/45000)
Validation | Loss: 0.211 | Acc: 93.22% (4661/5000)

Epoch 148/200
Train Batch: 50/352 | Loss: 0.161 | Acc: 94.73% (6063/6400)
Train Batch: 100/352 | Loss: 0.154 | Acc: 95.07% (12169/12800)
Train Batch: 150/352 | Loss: 0.156 | Acc: 94.83% (18207/19200)
Train Batch: 200/352 | Loss: 0.157 | Acc: 94.83% (24277/25600)
Train Batch: 250/352 | Loss: 0.156 | Acc: 94.88% (30361/32000)
Train Batch: 300/352 | Loss: 0.159 | Acc: 94.78% (36394/38400)
Train Batch: 350/352 | Loss: 0.162 | Acc: 94.70% (42424/44800)
Train Batch: 352/352 | Loss: 0.162 | Acc: 94.70% (42614/45000)
Validation | Loss: 0.207 | Acc: 93.36% (4668/5000)
New best model with accuracy: 0.9336

Epoch 149/200
Train Batch: 50/352 | Loss: 0.159 | Acc: 94.70% (6061/6400)
Train Batch: 100/352 | Loss: 0.150 | Acc: 95.03% (12164/12800)
Train Batch: 150/352 | Loss: 0.151 | Acc: 95.04% (18247/19200)
Train Batch: 200/352 | Loss: 0.151 | Acc: 95.01% (24323/25600)
Train Batch: 250/352 | Loss: 0.153 | Acc: 94.94% (30380/32000)
Train Batch: 300/352 | Loss: 0.154 | Acc: 94.86% (36426/38400)
Train Batch: 350/352 | Loss: 0.154 | Acc: 94.85% (42495/44800)
Train Batch: 352/352 | Loss: 0.154 | Acc: 94.86% (42688/45000)
Validation | Loss: 0.197 | Acc: 93.76% (4688/5000)
New best model with accuracy: 0.9376

Epoch 150/200
Train Batch: 50/352 | Loss: 0.148 | Acc: 95.23% (6095/6400)
Train Batch: 100/352 | Loss: 0.152 | Acc: 95.03% (12164/12800)
Train Batch: 150/352 | Loss: 0.153 | Acc: 94.95% (18230/19200)
Train Batch: 200/352 | Loss: 0.153 | Acc: 94.93% (24302/25600)
Train Batch: 250/352 | Loss: 0.153 | Acc: 94.86% (30356/32000)
Train Batch: 300/352 | Loss: 0.155 | Acc: 94.83% (36414/38400)
Train Batch: 350/352 | Loss: 0.156 | Acc: 94.78% (42462/44800)
Train Batch: 352/352 | Loss: 0.156 | Acc: 94.78% (42650/45000)
Validation | Loss: 0.235 | Acc: 92.46% (4623/5000)

Epoch 151/200
Train Batch: 50/352 | Loss: 0.152 | Acc: 94.91% (6074/6400)
Train Batch: 100/352 | Loss: 0.143 | Acc: 95.25% (12192/12800)
Train Batch: 150/352 | Loss: 0.141 | Acc: 95.37% (18311/19200)
Train Batch: 200/352 | Loss: 0.142 | Acc: 95.33% (24404/25600)
Train Batch: 250/352 | Loss: 0.145 | Acc: 95.21% (30467/32000)
Train Batch: 300/352 | Loss: 0.145 | Acc: 95.15% (36536/38400)
Train Batch: 350/352 | Loss: 0.145 | Acc: 95.19% (42646/44800)
Train Batch: 352/352 | Loss: 0.145 | Acc: 95.19% (42837/45000)
Validation | Loss: 0.192 | Acc: 93.60% (4680/5000)

Epoch 152/200
Train Batch: 50/352 | Loss: 0.136 | Acc: 95.28% (6098/6400)
Train Batch: 100/352 | Loss: 0.137 | Acc: 95.45% (12217/12800)
Train Batch: 150/352 | Loss: 0.137 | Acc: 95.41% (18318/19200)
Train Batch: 200/352 | Loss: 0.137 | Acc: 95.40% (24423/25600)
Train Batch: 250/352 | Loss: 0.140 | Acc: 95.34% (30509/32000)
Train Batch: 300/352 | Loss: 0.142 | Acc: 95.26% (36578/38400)
Train Batch: 350/352 | Loss: 0.144 | Acc: 95.18% (42642/44800)
Train Batch: 352/352 | Loss: 0.145 | Acc: 95.17% (42828/45000)
Validation | Loss: 0.202 | Acc: 93.80% (4690/5000)
New best model with accuracy: 0.9380

Epoch 153/200
Train Batch: 50/352 | Loss: 0.135 | Acc: 95.61% (6119/6400)
Train Batch: 100/352 | Loss: 0.137 | Acc: 95.50% (12224/12800)
Train Batch: 150/352 | Loss: 0.135 | Acc: 95.57% (18350/19200)
Train Batch: 200/352 | Loss: 0.136 | Acc: 95.51% (24450/25600)
Train Batch: 250/352 | Loss: 0.136 | Acc: 95.57% (30583/32000)
Train Batch: 300/352 | Loss: 0.137 | Acc: 95.52% (36679/38400)
Train Batch: 350/352 | Loss: 0.138 | Acc: 95.51% (42787/44800)
Train Batch: 352/352 | Loss: 0.138 | Acc: 95.50% (42977/45000)
Validation | Loss: 0.206 | Acc: 93.42% (4671/5000)

Epoch 154/200
Train Batch: 50/352 | Loss: 0.132 | Acc: 95.80% (6131/6400)
Train Batch: 100/352 | Loss: 0.129 | Acc: 95.93% (12279/12800)
Train Batch: 150/352 | Loss: 0.126 | Acc: 95.96% (18425/19200)
Train Batch: 200/352 | Loss: 0.128 | Acc: 95.86% (24539/25600)
Train Batch: 250/352 | Loss: 0.131 | Acc: 95.74% (30637/32000)
Train Batch: 300/352 | Loss: 0.131 | Acc: 95.73% (36762/38400)
Train Batch: 350/352 | Loss: 0.133 | Acc: 95.67% (42860/44800)
Train Batch: 352/352 | Loss: 0.133 | Acc: 95.66% (43049/45000)
Validation | Loss: 0.201 | Acc: 93.70% (4685/5000)

Epoch 155/200
Train Batch: 50/352 | Loss: 0.129 | Acc: 95.83% (6133/6400)
Train Batch: 100/352 | Loss: 0.135 | Acc: 95.56% (12232/12800)
Train Batch: 150/352 | Loss: 0.133 | Acc: 95.56% (18348/19200)
Train Batch: 200/352 | Loss: 0.131 | Acc: 95.65% (24487/25600)
Train Batch: 250/352 | Loss: 0.130 | Acc: 95.68% (30618/32000)
Train Batch: 300/352 | Loss: 0.129 | Acc: 95.74% (36766/38400)
Train Batch: 350/352 | Loss: 0.128 | Acc: 95.72% (42884/44800)
Train Batch: 352/352 | Loss: 0.128 | Acc: 95.72% (43075/45000)
Validation | Loss: 0.206 | Acc: 93.60% (4680/5000)

Epoch 156/200
Train Batch: 50/352 | Loss: 0.135 | Acc: 95.62% (6120/6400)
Train Batch: 100/352 | Loss: 0.129 | Acc: 95.67% (12246/12800)
Train Batch: 150/352 | Loss: 0.129 | Acc: 95.68% (18371/19200)
Train Batch: 200/352 | Loss: 0.129 | Acc: 95.67% (24491/25600)
Train Batch: 250/352 | Loss: 0.126 | Acc: 95.79% (30652/32000)
Train Batch: 300/352 | Loss: 0.124 | Acc: 95.91% (36831/38400)
Train Batch: 350/352 | Loss: 0.124 | Acc: 95.88% (42954/44800)
Train Batch: 352/352 | Loss: 0.124 | Acc: 95.88% (43145/45000)
Validation | Loss: 0.190 | Acc: 94.32% (4716/5000)
New best model with accuracy: 0.9432

Epoch 157/200
Train Batch: 50/352 | Loss: 0.111 | Acc: 96.36% (6167/6400)
Train Batch: 100/352 | Loss: 0.115 | Acc: 96.38% (12336/12800)
Train Batch: 150/352 | Loss: 0.116 | Acc: 96.34% (18498/19200)
Train Batch: 200/352 | Loss: 0.114 | Acc: 96.38% (24673/25600)
Train Batch: 250/352 | Loss: 0.115 | Acc: 96.34% (30830/32000)
Train Batch: 300/352 | Loss: 0.115 | Acc: 96.31% (36984/38400)
Train Batch: 350/352 | Loss: 0.116 | Acc: 96.30% (43141/44800)
Train Batch: 352/352 | Loss: 0.116 | Acc: 96.30% (43334/45000)
Validation | Loss: 0.180 | Acc: 94.12% (4706/5000)

Epoch 158/200
Train Batch: 50/352 | Loss: 0.112 | Acc: 96.58% (6181/6400)
Train Batch: 100/352 | Loss: 0.112 | Acc: 96.37% (12335/12800)
Train Batch: 150/352 | Loss: 0.111 | Acc: 96.38% (18504/19200)
Train Batch: 200/352 | Loss: 0.113 | Acc: 96.29% (24651/25600)
Train Batch: 250/352 | Loss: 0.117 | Acc: 96.17% (30773/32000)
Train Batch: 300/352 | Loss: 0.117 | Acc: 96.15% (36923/38400)
Train Batch: 350/352 | Loss: 0.116 | Acc: 96.23% (43111/44800)
Train Batch: 352/352 | Loss: 0.116 | Acc: 96.23% (43304/45000)
Validation | Loss: 0.196 | Acc: 94.36% (4718/5000)
New best model with accuracy: 0.9436

Epoch 159/200
Train Batch: 50/352 | Loss: 0.104 | Acc: 96.81% (6196/6400)
Train Batch: 100/352 | Loss: 0.104 | Acc: 96.79% (12389/12800)
Train Batch: 150/352 | Loss: 0.102 | Acc: 96.82% (18590/19200)
Train Batch: 200/352 | Loss: 0.105 | Acc: 96.64% (24741/25600)
Train Batch: 250/352 | Loss: 0.104 | Acc: 96.71% (30948/32000)
Train Batch: 300/352 | Loss: 0.106 | Acc: 96.62% (37104/38400)
Train Batch: 350/352 | Loss: 0.107 | Acc: 96.54% (43251/44800)
Train Batch: 352/352 | Loss: 0.107 | Acc: 96.54% (43442/45000)
Validation | Loss: 0.184 | Acc: 94.10% (4705/5000)

Epoch 160/200
Train Batch: 50/352 | Loss: 0.096 | Acc: 96.89% (6201/6400)
Train Batch: 100/352 | Loss: 0.099 | Acc: 96.79% (12389/12800)
Train Batch: 150/352 | Loss: 0.101 | Acc: 96.74% (18575/19200)
Train Batch: 200/352 | Loss: 0.103 | Acc: 96.63% (24737/25600)
Train Batch: 250/352 | Loss: 0.105 | Acc: 96.51% (30882/32000)
Train Batch: 300/352 | Loss: 0.107 | Acc: 96.46% (37040/38400)
Train Batch: 350/352 | Loss: 0.107 | Acc: 96.45% (43208/44800)
Train Batch: 352/352 | Loss: 0.107 | Acc: 96.44% (43400/45000)
Validation | Loss: 0.181 | Acc: 94.28% (4714/5000)

Epoch 161/200
Train Batch: 50/352 | Loss: 0.097 | Acc: 96.92% (6203/6400)
Train Batch: 100/352 | Loss: 0.099 | Acc: 96.80% (12390/12800)
Train Batch: 150/352 | Loss: 0.101 | Acc: 96.70% (18567/19200)
Train Batch: 200/352 | Loss: 0.099 | Acc: 96.75% (24769/25600)
Train Batch: 250/352 | Loss: 0.099 | Acc: 96.75% (30960/32000)
Train Batch: 300/352 | Loss: 0.099 | Acc: 96.71% (37138/38400)
Train Batch: 350/352 | Loss: 0.100 | Acc: 96.72% (43330/44800)
Train Batch: 352/352 | Loss: 0.100 | Acc: 96.71% (43521/45000)
Validation | Loss: 0.191 | Acc: 93.98% (4699/5000)

Epoch 162/200
Train Batch: 50/352 | Loss: 0.093 | Acc: 97.06% (6212/6400)
Train Batch: 100/352 | Loss: 0.096 | Acc: 96.95% (12409/12800)
Train Batch: 150/352 | Loss: 0.095 | Acc: 97.03% (18629/19200)
Train Batch: 200/352 | Loss: 0.094 | Acc: 97.05% (24844/25600)
Train Batch: 250/352 | Loss: 0.093 | Acc: 97.05% (31056/32000)
Train Batch: 300/352 | Loss: 0.094 | Acc: 96.99% (37243/38400)
Train Batch: 350/352 | Loss: 0.095 | Acc: 96.96% (43437/44800)
Train Batch: 352/352 | Loss: 0.096 | Acc: 96.94% (43625/45000)
Validation | Loss: 0.167 | Acc: 94.96% (4748/5000)
New best model with accuracy: 0.9496

Epoch 163/200
Train Batch: 50/352 | Loss: 0.087 | Acc: 97.23% (6223/6400)
Train Batch: 100/352 | Loss: 0.089 | Acc: 97.20% (12442/12800)
Train Batch: 150/352 | Loss: 0.092 | Acc: 97.03% (18630/19200)
Train Batch: 200/352 | Loss: 0.092 | Acc: 96.98% (24827/25600)
Train Batch: 250/352 | Loss: 0.093 | Acc: 96.98% (31033/32000)
Train Batch: 300/352 | Loss: 0.094 | Acc: 96.98% (37239/38400)
Train Batch: 350/352 | Loss: 0.094 | Acc: 96.98% (43445/44800)
Train Batch: 352/352 | Loss: 0.094 | Acc: 96.98% (43639/45000)
Validation | Loss: 0.182 | Acc: 94.56% (4728/5000)

Epoch 164/200
Train Batch: 50/352 | Loss: 0.089 | Acc: 97.20% (6221/6400)
Train Batch: 100/352 | Loss: 0.088 | Acc: 97.25% (12448/12800)
Train Batch: 150/352 | Loss: 0.087 | Acc: 97.25% (18672/19200)
Train Batch: 200/352 | Loss: 0.087 | Acc: 97.24% (24894/25600)
Train Batch: 250/352 | Loss: 0.085 | Acc: 97.31% (31138/32000)
Train Batch: 300/352 | Loss: 0.085 | Acc: 97.34% (37378/38400)
Train Batch: 350/352 | Loss: 0.085 | Acc: 97.31% (43593/44800)
Train Batch: 352/352 | Loss: 0.085 | Acc: 97.30% (43786/45000)
Validation | Loss: 0.187 | Acc: 94.24% (4712/5000)

Epoch 165/200
Train Batch: 50/352 | Loss: 0.084 | Acc: 97.25% (6224/6400)
Train Batch: 100/352 | Loss: 0.084 | Acc: 97.20% (12442/12800)
Train Batch: 150/352 | Loss: 0.081 | Acc: 97.26% (18674/19200)
Train Batch: 200/352 | Loss: 0.081 | Acc: 97.27% (24902/25600)
Train Batch: 250/352 | Loss: 0.082 | Acc: 97.18% (31099/32000)
Train Batch: 300/352 | Loss: 0.083 | Acc: 97.19% (37320/38400)
Train Batch: 350/352 | Loss: 0.083 | Acc: 97.19% (43540/44800)
Train Batch: 352/352 | Loss: 0.083 | Acc: 97.19% (43734/45000)
Validation | Loss: 0.184 | Acc: 94.68% (4734/5000)

Epoch 166/200
Train Batch: 50/352 | Loss: 0.076 | Acc: 97.55% (6243/6400)
Train Batch: 100/352 | Loss: 0.079 | Acc: 97.49% (12479/12800)
Train Batch: 150/352 | Loss: 0.078 | Acc: 97.52% (18723/19200)
Train Batch: 200/352 | Loss: 0.079 | Acc: 97.47% (24952/25600)
Train Batch: 250/352 | Loss: 0.079 | Acc: 97.42% (31176/32000)
Train Batch: 300/352 | Loss: 0.079 | Acc: 97.40% (37401/38400)
Train Batch: 350/352 | Loss: 0.080 | Acc: 97.35% (43615/44800)
Train Batch: 352/352 | Loss: 0.080 | Acc: 97.36% (43811/45000)
Validation | Loss: 0.185 | Acc: 94.44% (4722/5000)

Epoch 167/200
Train Batch: 50/352 | Loss: 0.071 | Acc: 97.67% (6251/6400)
Train Batch: 100/352 | Loss: 0.071 | Acc: 97.74% (12511/12800)
Train Batch: 150/352 | Loss: 0.073 | Acc: 97.64% (18746/19200)
Train Batch: 200/352 | Loss: 0.073 | Acc: 97.69% (25009/25600)
Train Batch: 250/352 | Loss: 0.074 | Acc: 97.64% (31245/32000)
Train Batch: 300/352 | Loss: 0.074 | Acc: 97.66% (37500/38400)
Train Batch: 350/352 | Loss: 0.074 | Acc: 97.65% (43745/44800)
Train Batch: 352/352 | Loss: 0.074 | Acc: 97.64% (43940/45000)
Validation | Loss: 0.170 | Acc: 94.86% (4743/5000)

Epoch 168/200
Train Batch: 50/352 | Loss: 0.073 | Acc: 97.53% (6242/6400)
Train Batch: 100/352 | Loss: 0.073 | Acc: 97.59% (12492/12800)
Train Batch: 150/352 | Loss: 0.071 | Acc: 97.67% (18753/19200)
Train Batch: 200/352 | Loss: 0.071 | Acc: 97.68% (25005/25600)
Train Batch: 250/352 | Loss: 0.072 | Acc: 97.70% (31264/32000)
Train Batch: 300/352 | Loss: 0.072 | Acc: 97.72% (37526/38400)
Train Batch: 350/352 | Loss: 0.072 | Acc: 97.72% (43778/44800)
Train Batch: 352/352 | Loss: 0.072 | Acc: 97.72% (43973/45000)
Validation | Loss: 0.185 | Acc: 94.46% (4723/5000)

Epoch 169/200
Train Batch: 50/352 | Loss: 0.062 | Acc: 98.22% (6286/6400)
Train Batch: 100/352 | Loss: 0.062 | Acc: 98.19% (12568/12800)
Train Batch: 150/352 | Loss: 0.065 | Acc: 98.05% (18826/19200)
Train Batch: 200/352 | Loss: 0.066 | Acc: 97.99% (25085/25600)
Train Batch: 250/352 | Loss: 0.067 | Acc: 97.93% (31337/32000)
Train Batch: 300/352 | Loss: 0.068 | Acc: 97.86% (37579/38400)
Train Batch: 350/352 | Loss: 0.069 | Acc: 97.82% (43822/44800)
Train Batch: 352/352 | Loss: 0.069 | Acc: 97.82% (44021/45000)
Validation | Loss: 0.180 | Acc: 94.38% (4719/5000)

Epoch 170/200
Train Batch: 50/352 | Loss: 0.061 | Acc: 98.12% (6280/6400)
Train Batch: 100/352 | Loss: 0.061 | Acc: 98.13% (12561/12800)
Train Batch: 150/352 | Loss: 0.063 | Acc: 98.09% (18834/19200)
Train Batch: 200/352 | Loss: 0.063 | Acc: 98.03% (25095/25600)
Train Batch: 250/352 | Loss: 0.063 | Acc: 98.07% (31381/32000)
Train Batch: 300/352 | Loss: 0.065 | Acc: 97.96% (37617/38400)
Train Batch: 350/352 | Loss: 0.066 | Acc: 97.91% (43865/44800)
Train Batch: 352/352 | Loss: 0.066 | Acc: 97.92% (44062/45000)
Validation | Loss: 0.187 | Acc: 94.70% (4735/5000)

Epoch 171/200
Train Batch: 50/352 | Loss: 0.059 | Acc: 98.27% (6289/6400)
Train Batch: 100/352 | Loss: 0.059 | Acc: 98.16% (12564/12800)
Train Batch: 150/352 | Loss: 0.058 | Acc: 98.19% (18852/19200)
Train Batch: 200/352 | Loss: 0.058 | Acc: 98.16% (25130/25600)
Train Batch: 250/352 | Loss: 0.059 | Acc: 98.16% (31412/32000)
Train Batch: 300/352 | Loss: 0.059 | Acc: 98.15% (37688/38400)
Train Batch: 350/352 | Loss: 0.060 | Acc: 98.09% (43946/44800)
Train Batch: 352/352 | Loss: 0.060 | Acc: 98.09% (44142/45000)
Validation | Loss: 0.188 | Acc: 94.80% (4740/5000)

Epoch 172/200
Train Batch: 50/352 | Loss: 0.055 | Acc: 98.41% (6298/6400)
Train Batch: 100/352 | Loss: 0.056 | Acc: 98.33% (12586/12800)
Train Batch: 150/352 | Loss: 0.057 | Acc: 98.31% (18876/19200)
Train Batch: 200/352 | Loss: 0.057 | Acc: 98.30% (25165/25600)
Train Batch: 250/352 | Loss: 0.055 | Acc: 98.38% (31481/32000)
Train Batch: 300/352 | Loss: 0.056 | Acc: 98.31% (37750/38400)
Train Batch: 350/352 | Loss: 0.057 | Acc: 98.30% (44039/44800)
Train Batch: 352/352 | Loss: 0.057 | Acc: 98.30% (44236/45000)
Validation | Loss: 0.171 | Acc: 94.90% (4745/5000)

Epoch 173/200
Train Batch: 50/352 | Loss: 0.047 | Acc: 98.58% (6309/6400)
Train Batch: 100/352 | Loss: 0.049 | Acc: 98.60% (12621/12800)
Train Batch: 150/352 | Loss: 0.050 | Acc: 98.52% (18916/19200)
Train Batch: 200/352 | Loss: 0.050 | Acc: 98.52% (25220/25600)
Train Batch: 250/352 | Loss: 0.049 | Acc: 98.50% (31520/32000)
Train Batch: 300/352 | Loss: 0.051 | Acc: 98.42% (37795/38400)
Train Batch: 350/352 | Loss: 0.051 | Acc: 98.42% (44091/44800)
Train Batch: 352/352 | Loss: 0.051 | Acc: 98.42% (44289/45000)
Validation | Loss: 0.178 | Acc: 95.06% (4753/5000)
New best model with accuracy: 0.9506

Epoch 174/200
Train Batch: 50/352 | Loss: 0.044 | Acc: 98.73% (6319/6400)
Train Batch: 100/352 | Loss: 0.047 | Acc: 98.53% (12612/12800)
Train Batch: 150/352 | Loss: 0.048 | Acc: 98.48% (18909/19200)
Train Batch: 200/352 | Loss: 0.048 | Acc: 98.50% (25217/25600)
Train Batch: 250/352 | Loss: 0.049 | Acc: 98.49% (31516/32000)
Train Batch: 300/352 | Loss: 0.049 | Acc: 98.52% (37830/38400)
Train Batch: 350/352 | Loss: 0.049 | Acc: 98.52% (44136/44800)
Train Batch: 352/352 | Loss: 0.050 | Acc: 98.52% (44332/45000)
Validation | Loss: 0.167 | Acc: 95.36% (4768/5000)
New best model with accuracy: 0.9536

Epoch 175/200
Train Batch: 50/352 | Loss: 0.050 | Acc: 98.64% (6313/6400)
Train Batch: 100/352 | Loss: 0.050 | Acc: 98.64% (12626/12800)
Train Batch: 150/352 | Loss: 0.047 | Acc: 98.65% (18940/19200)
Train Batch: 200/352 | Loss: 0.047 | Acc: 98.61% (25244/25600)
Train Batch: 250/352 | Loss: 0.046 | Acc: 98.61% (31554/32000)
Train Batch: 300/352 | Loss: 0.046 | Acc: 98.61% (37868/38400)
Train Batch: 350/352 | Loss: 0.046 | Acc: 98.62% (44184/44800)
Train Batch: 352/352 | Loss: 0.046 | Acc: 98.63% (44383/45000)
Validation | Loss: 0.177 | Acc: 95.32% (4766/5000)

Epoch 176/200
Train Batch: 50/352 | Loss: 0.049 | Acc: 98.50% (6304/6400)
Train Batch: 100/352 | Loss: 0.046 | Acc: 98.56% (12616/12800)
Train Batch: 150/352 | Loss: 0.046 | Acc: 98.62% (18935/19200)
Train Batch: 200/352 | Loss: 0.046 | Acc: 98.63% (25250/25600)
Train Batch: 250/352 | Loss: 0.046 | Acc: 98.63% (31563/32000)
Train Batch: 300/352 | Loss: 0.047 | Acc: 98.55% (37843/38400)
Train Batch: 350/352 | Loss: 0.047 | Acc: 98.57% (44161/44800)
Train Batch: 352/352 | Loss: 0.047 | Acc: 98.57% (44357/45000)
Validation | Loss: 0.167 | Acc: 95.56% (4778/5000)
New best model with accuracy: 0.9556

Epoch 177/200
Train Batch: 50/352 | Loss: 0.036 | Acc: 98.97% (6334/6400)
Train Batch: 100/352 | Loss: 0.038 | Acc: 98.90% (12659/12800)
Train Batch: 150/352 | Loss: 0.040 | Acc: 98.86% (18981/19200)
Train Batch: 200/352 | Loss: 0.039 | Acc: 98.86% (25308/25600)
Train Batch: 250/352 | Loss: 0.039 | Acc: 98.83% (31627/32000)
Train Batch: 300/352 | Loss: 0.039 | Acc: 98.83% (37951/38400)
Train Batch: 350/352 | Loss: 0.039 | Acc: 98.84% (44281/44800)
Train Batch: 352/352 | Loss: 0.039 | Acc: 98.84% (44476/45000)
Validation | Loss: 0.169 | Acc: 95.26% (4763/5000)

Epoch 178/200
Train Batch: 50/352 | Loss: 0.034 | Acc: 99.09% (6342/6400)
Train Batch: 100/352 | Loss: 0.036 | Acc: 99.04% (12677/12800)
Train Batch: 150/352 | Loss: 0.039 | Acc: 98.93% (18995/19200)
Train Batch: 200/352 | Loss: 0.041 | Acc: 98.85% (25305/25600)
Train Batch: 250/352 | Loss: 0.040 | Acc: 98.88% (31641/32000)
Train Batch: 300/352 | Loss: 0.041 | Acc: 98.83% (37950/38400)
Train Batch: 350/352 | Loss: 0.042 | Acc: 98.77% (44247/44800)
Train Batch: 352/352 | Loss: 0.042 | Acc: 98.77% (44445/45000)
Validation | Loss: 0.166 | Acc: 95.26% (4763/5000)

Epoch 179/200
Train Batch: 50/352 | Loss: 0.041 | Acc: 98.72% (6318/6400)
Train Batch: 100/352 | Loss: 0.039 | Acc: 98.77% (12642/12800)
Train Batch: 150/352 | Loss: 0.038 | Acc: 98.87% (18983/19200)
Train Batch: 200/352 | Loss: 0.038 | Acc: 98.88% (25313/25600)
Train Batch: 250/352 | Loss: 0.037 | Acc: 98.88% (31643/32000)
Train Batch: 300/352 | Loss: 0.038 | Acc: 98.89% (37973/38400)
Train Batch: 350/352 | Loss: 0.037 | Acc: 98.91% (44313/44800)
Train Batch: 352/352 | Loss: 0.038 | Acc: 98.91% (44508/45000)
Validation | Loss: 0.170 | Acc: 95.14% (4757/5000)

Epoch 180/200
Train Batch: 50/352 | Loss: 0.035 | Acc: 98.89% (6329/6400)
Train Batch: 100/352 | Loss: 0.035 | Acc: 98.94% (12664/12800)
Train Batch: 150/352 | Loss: 0.035 | Acc: 98.98% (19005/19200)
Train Batch: 200/352 | Loss: 0.034 | Acc: 99.02% (25350/25600)
Train Batch: 250/352 | Loss: 0.034 | Acc: 99.06% (31699/32000)
Train Batch: 300/352 | Loss: 0.034 | Acc: 99.05% (38035/38400)
Train Batch: 350/352 | Loss: 0.034 | Acc: 99.03% (44367/44800)
Train Batch: 352/352 | Loss: 0.034 | Acc: 99.04% (44566/45000)
Validation | Loss: 0.161 | Acc: 95.52% (4776/5000)

Epoch 181/200
Train Batch: 50/352 | Loss: 0.033 | Acc: 99.05% (6339/6400)
Train Batch: 100/352 | Loss: 0.033 | Acc: 99.05% (12678/12800)
Train Batch: 150/352 | Loss: 0.033 | Acc: 99.04% (19015/19200)
Train Batch: 200/352 | Loss: 0.033 | Acc: 99.04% (25353/25600)
Train Batch: 250/352 | Loss: 0.034 | Acc: 99.00% (31681/32000)
Train Batch: 300/352 | Loss: 0.034 | Acc: 99.00% (38015/38400)
Train Batch: 350/352 | Loss: 0.034 | Acc: 99.00% (44352/44800)
Train Batch: 352/352 | Loss: 0.034 | Acc: 99.00% (44551/45000)
Validation | Loss: 0.166 | Acc: 95.40% (4770/5000)

Epoch 182/200
Train Batch: 50/352 | Loss: 0.031 | Acc: 99.06% (6340/6400)
Train Batch: 100/352 | Loss: 0.032 | Acc: 99.09% (12683/12800)
Train Batch: 150/352 | Loss: 0.031 | Acc: 99.16% (19038/19200)
Train Batch: 200/352 | Loss: 0.033 | Acc: 99.09% (25367/25600)
Train Batch: 250/352 | Loss: 0.033 | Acc: 99.08% (31705/32000)
Train Batch: 300/352 | Loss: 0.033 | Acc: 99.08% (38045/38400)
Train Batch: 350/352 | Loss: 0.032 | Acc: 99.08% (44387/44800)
Train Batch: 352/352 | Loss: 0.032 | Acc: 99.08% (44585/45000)
Validation | Loss: 0.164 | Acc: 95.48% (4774/5000)

Epoch 183/200
Train Batch: 50/352 | Loss: 0.031 | Acc: 99.16% (6346/6400)
Train Batch: 100/352 | Loss: 0.030 | Acc: 99.12% (12688/12800)
Train Batch: 150/352 | Loss: 0.030 | Acc: 99.18% (19042/19200)
Train Batch: 200/352 | Loss: 0.030 | Acc: 99.19% (25393/25600)
Train Batch: 250/352 | Loss: 0.029 | Acc: 99.22% (31749/32000)
Train Batch: 300/352 | Loss: 0.030 | Acc: 99.20% (38093/38400)
Train Batch: 350/352 | Loss: 0.029 | Acc: 99.20% (44441/44800)
Train Batch: 352/352 | Loss: 0.029 | Acc: 99.20% (44641/45000)
Validation | Loss: 0.167 | Acc: 95.50% (4775/5000)

Epoch 184/200
Train Batch: 50/352 | Loss: 0.029 | Acc: 99.20% (6349/6400)
Train Batch: 100/352 | Loss: 0.031 | Acc: 99.15% (12691/12800)
Train Batch: 150/352 | Loss: 0.031 | Acc: 99.13% (19033/19200)
Train Batch: 200/352 | Loss: 0.030 | Acc: 99.15% (25383/25600)
Train Batch: 250/352 | Loss: 0.030 | Acc: 99.17% (31733/32000)
Train Batch: 300/352 | Loss: 0.030 | Acc: 99.17% (38082/38400)
Train Batch: 350/352 | Loss: 0.030 | Acc: 99.20% (44440/44800)
Train Batch: 352/352 | Loss: 0.030 | Acc: 99.20% (44640/45000)
Validation | Loss: 0.168 | Acc: 95.32% (4766/5000)

Epoch 185/200
Train Batch: 50/352 | Loss: 0.027 | Acc: 99.31% (6356/6400)
Train Batch: 100/352 | Loss: 0.027 | Acc: 99.28% (12708/12800)
Train Batch: 150/352 | Loss: 0.028 | Acc: 99.26% (19058/19200)
Train Batch: 200/352 | Loss: 0.028 | Acc: 99.29% (25417/25600)
Train Batch: 250/352 | Loss: 0.028 | Acc: 99.26% (31763/32000)
Train Batch: 300/352 | Loss: 0.027 | Acc: 99.30% (38131/38400)
Train Batch: 350/352 | Loss: 0.027 | Acc: 99.30% (44488/44800)
Train Batch: 352/352 | Loss: 0.027 | Acc: 99.30% (44686/45000)
Validation | Loss: 0.166 | Acc: 95.36% (4768/5000)

Epoch 186/200
Train Batch: 50/352 | Loss: 0.025 | Acc: 99.31% (6356/6400)
Train Batch: 100/352 | Loss: 0.025 | Acc: 99.35% (12717/12800)
Train Batch: 150/352 | Loss: 0.026 | Acc: 99.30% (19065/19200)
Train Batch: 200/352 | Loss: 0.026 | Acc: 99.29% (25419/25600)
Train Batch: 250/352 | Loss: 0.026 | Acc: 99.30% (31776/32000)
Train Batch: 300/352 | Loss: 0.026 | Acc: 99.29% (38129/38400)
Train Batch: 350/352 | Loss: 0.026 | Acc: 99.31% (44489/44800)
Train Batch: 352/352 | Loss: 0.026 | Acc: 99.30% (44686/45000)
Validation | Loss: 0.170 | Acc: 95.40% (4770/5000)

Epoch 187/200
Train Batch: 50/352 | Loss: 0.027 | Acc: 99.30% (6355/6400)
Train Batch: 100/352 | Loss: 0.026 | Acc: 99.31% (12712/12800)
Train Batch: 150/352 | Loss: 0.027 | Acc: 99.23% (19053/19200)
Train Batch: 200/352 | Loss: 0.026 | Acc: 99.31% (25423/25600)
Train Batch: 250/352 | Loss: 0.025 | Acc: 99.31% (31780/32000)
Train Batch: 300/352 | Loss: 0.026 | Acc: 99.27% (38121/38400)
Train Batch: 350/352 | Loss: 0.026 | Acc: 99.29% (44480/44800)
Train Batch: 352/352 | Loss: 0.026 | Acc: 99.29% (44679/45000)
Validation | Loss: 0.168 | Acc: 95.70% (4785/5000)
New best model with accuracy: 0.9570

Epoch 188/200
Train Batch: 50/352 | Loss: 0.027 | Acc: 99.28% (6354/6400)
Train Batch: 100/352 | Loss: 0.027 | Acc: 99.23% (12701/12800)
Train Batch: 150/352 | Loss: 0.025 | Acc: 99.31% (19067/19200)
Train Batch: 200/352 | Loss: 0.025 | Acc: 99.31% (25423/25600)
Train Batch: 250/352 | Loss: 0.025 | Acc: 99.33% (31786/32000)
Train Batch: 300/352 | Loss: 0.025 | Acc: 99.35% (38149/38400)
Train Batch: 350/352 | Loss: 0.025 | Acc: 99.36% (44512/44800)
Train Batch: 352/352 | Loss: 0.025 | Acc: 99.36% (44712/45000)
Validation | Loss: 0.162 | Acc: 95.62% (4781/5000)

Epoch 189/200
Train Batch: 50/352 | Loss: 0.024 | Acc: 99.33% (6357/6400)
Train Batch: 100/352 | Loss: 0.023 | Acc: 99.36% (12718/12800)
Train Batch: 150/352 | Loss: 0.023 | Acc: 99.36% (19078/19200)
Train Batch: 200/352 | Loss: 0.023 | Acc: 99.38% (25440/25600)
Train Batch: 250/352 | Loss: 0.023 | Acc: 99.38% (31801/32000)
Train Batch: 300/352 | Loss: 0.024 | Acc: 99.36% (38153/38400)
Train Batch: 350/352 | Loss: 0.024 | Acc: 99.36% (44514/44800)
Train Batch: 352/352 | Loss: 0.024 | Acc: 99.36% (44712/45000)
Validation | Loss: 0.164 | Acc: 95.54% (4777/5000)

Epoch 190/200
Train Batch: 50/352 | Loss: 0.026 | Acc: 99.25% (6352/6400)
Train Batch: 100/352 | Loss: 0.026 | Acc: 99.27% (12706/12800)
Train Batch: 150/352 | Loss: 0.024 | Acc: 99.31% (19068/19200)
Train Batch: 200/352 | Loss: 0.024 | Acc: 99.34% (25430/25600)
Train Batch: 250/352 | Loss: 0.023 | Acc: 99.38% (31802/32000)
Train Batch: 300/352 | Loss: 0.022 | Acc: 99.41% (38172/38400)
Train Batch: 350/352 | Loss: 0.022 | Acc: 99.41% (44536/44800)
Train Batch: 352/352 | Loss: 0.022 | Acc: 99.41% (44733/45000)
Validation | Loss: 0.162 | Acc: 95.62% (4781/5000)

Epoch 191/200
Train Batch: 50/352 | Loss: 0.021 | Acc: 99.47% (6366/6400)
Train Batch: 100/352 | Loss: 0.023 | Acc: 99.40% (12723/12800)
Train Batch: 150/352 | Loss: 0.023 | Acc: 99.37% (19079/19200)
Train Batch: 200/352 | Loss: 0.024 | Acc: 99.35% (25433/25600)
Train Batch: 250/352 | Loss: 0.023 | Acc: 99.35% (31793/32000)
Train Batch: 300/352 | Loss: 0.023 | Acc: 99.36% (38155/38400)
Train Batch: 350/352 | Loss: 0.023 | Acc: 99.36% (44512/44800)
Train Batch: 352/352 | Loss: 0.023 | Acc: 99.36% (44712/45000)
Validation | Loss: 0.166 | Acc: 95.60% (4780/5000)

Epoch 192/200
Train Batch: 50/352 | Loss: 0.021 | Acc: 99.45% (6365/6400)
Train Batch: 100/352 | Loss: 0.021 | Acc: 99.41% (12725/12800)
Train Batch: 150/352 | Loss: 0.022 | Acc: 99.37% (19079/19200)
Train Batch: 200/352 | Loss: 0.022 | Acc: 99.37% (25439/25600)
Train Batch: 250/352 | Loss: 0.023 | Acc: 99.36% (31794/32000)
Train Batch: 300/352 | Loss: 0.023 | Acc: 99.37% (38157/38400)
Train Batch: 350/352 | Loss: 0.022 | Acc: 99.40% (44529/44800)
Train Batch: 352/352 | Loss: 0.022 | Acc: 99.39% (44727/45000)
Validation | Loss: 0.165 | Acc: 95.64% (4782/5000)

Epoch 193/200
Train Batch: 50/352 | Loss: 0.021 | Acc: 99.55% (6371/6400)
Train Batch: 100/352 | Loss: 0.023 | Acc: 99.47% (12732/12800)
Train Batch: 150/352 | Loss: 0.024 | Acc: 99.47% (19098/19200)
Train Batch: 200/352 | Loss: 0.023 | Acc: 99.47% (25465/25600)
Train Batch: 250/352 | Loss: 0.023 | Acc: 99.44% (31821/32000)
Train Batch: 300/352 | Loss: 0.022 | Acc: 99.45% (38190/38400)
Train Batch: 350/352 | Loss: 0.022 | Acc: 99.44% (44551/44800)
Train Batch: 352/352 | Loss: 0.023 | Acc: 99.44% (44747/45000)
Validation | Loss: 0.165 | Acc: 95.60% (4780/5000)

Epoch 194/200
Train Batch: 50/352 | Loss: 0.020 | Acc: 99.56% (6372/6400)
Train Batch: 100/352 | Loss: 0.020 | Acc: 99.50% (12736/12800)
Train Batch: 150/352 | Loss: 0.020 | Acc: 99.53% (19110/19200)
Train Batch: 200/352 | Loss: 0.021 | Acc: 99.50% (25472/25600)
Train Batch: 250/352 | Loss: 0.021 | Acc: 99.47% (31832/32000)
Train Batch: 300/352 | Loss: 0.021 | Acc: 99.48% (38200/38400)
Train Batch: 350/352 | Loss: 0.021 | Acc: 99.48% (44568/44800)
Train Batch: 352/352 | Loss: 0.021 | Acc: 99.48% (44768/45000)
Validation | Loss: 0.167 | Acc: 95.66% (4783/5000)

Epoch 195/200
Train Batch: 50/352 | Loss: 0.023 | Acc: 99.53% (6370/6400)
Train Batch: 100/352 | Loss: 0.022 | Acc: 99.51% (12737/12800)
Train Batch: 150/352 | Loss: 0.022 | Acc: 99.51% (19106/19200)
Train Batch: 200/352 | Loss: 0.022 | Acc: 99.50% (25473/25600)
Train Batch: 250/352 | Loss: 0.021 | Acc: 99.50% (31841/32000)
Train Batch: 300/352 | Loss: 0.021 | Acc: 99.50% (38207/38400)
Train Batch: 350/352 | Loss: 0.022 | Acc: 99.48% (44569/44800)
Train Batch: 352/352 | Loss: 0.022 | Acc: 99.48% (44767/45000)
Validation | Loss: 0.164 | Acc: 95.58% (4779/5000)

Epoch 196/200
Train Batch: 50/352 | Loss: 0.022 | Acc: 99.48% (6367/6400)
Train Batch: 100/352 | Loss: 0.021 | Acc: 99.51% (12737/12800)
Train Batch: 150/352 | Loss: 0.021 | Acc: 99.54% (19111/19200)
Train Batch: 200/352 | Loss: 0.021 | Acc: 99.50% (25471/25600)
Train Batch: 250/352 | Loss: 0.021 | Acc: 99.52% (31845/32000)
Train Batch: 300/352 | Loss: 0.021 | Acc: 99.51% (38212/38400)
Train Batch: 350/352 | Loss: 0.020 | Acc: 99.51% (44582/44800)
Train Batch: 352/352 | Loss: 0.021 | Acc: 99.51% (44778/45000)
Validation | Loss: 0.164 | Acc: 95.70% (4785/5000)

Epoch 197/200
Train Batch: 50/352 | Loss: 0.020 | Acc: 99.66% (6378/6400)
Train Batch: 100/352 | Loss: 0.020 | Acc: 99.55% (12742/12800)
Train Batch: 150/352 | Loss: 0.021 | Acc: 99.50% (19104/19200)
Train Batch: 200/352 | Loss: 0.020 | Acc: 99.53% (25479/25600)
Train Batch: 250/352 | Loss: 0.021 | Acc: 99.53% (31849/32000)
Train Batch: 300/352 | Loss: 0.021 | Acc: 99.53% (38219/38400)
Train Batch: 350/352 | Loss: 0.020 | Acc: 99.54% (44593/44800)
Train Batch: 352/352 | Loss: 0.020 | Acc: 99.54% (44792/45000)
Validation | Loss: 0.164 | Acc: 95.68% (4784/5000)

Epoch 198/200
Train Batch: 50/352 | Loss: 0.019 | Acc: 99.59% (6374/6400)
Train Batch: 100/352 | Loss: 0.020 | Acc: 99.48% (12733/12800)
Train Batch: 150/352 | Loss: 0.020 | Acc: 99.49% (19103/19200)
Train Batch: 200/352 | Loss: 0.020 | Acc: 99.51% (25475/25600)
Train Batch: 250/352 | Loss: 0.020 | Acc: 99.50% (31839/32000)
Train Batch: 300/352 | Loss: 0.020 | Acc: 99.50% (38209/38400)
Train Batch: 350/352 | Loss: 0.020 | Acc: 99.51% (44582/44800)
Train Batch: 352/352 | Loss: 0.020 | Acc: 99.52% (44782/45000)
Validation | Loss: 0.165 | Acc: 95.80% (4790/5000)
New best model with accuracy: 0.9580

Epoch 199/200
Train Batch: 50/352 | Loss: 0.021 | Acc: 99.44% (6364/6400)
Train Batch: 100/352 | Loss: 0.019 | Acc: 99.52% (12739/12800)
Train Batch: 150/352 | Loss: 0.020 | Acc: 99.54% (19111/19200)
Train Batch: 200/352 | Loss: 0.020 | Acc: 99.55% (25485/25600)
Train Batch: 250/352 | Loss: 0.020 | Acc: 99.55% (31857/32000)
Train Batch: 300/352 | Loss: 0.020 | Acc: 99.52% (38217/38400)
Train Batch: 350/352 | Loss: 0.020 | Acc: 99.53% (44590/44800)
Train Batch: 352/352 | Loss: 0.020 | Acc: 99.53% (44787/45000)
Validation | Loss: 0.163 | Acc: 95.68% (4784/5000)

Epoch 200/200
Train Batch: 50/352 | Loss: 0.024 | Acc: 99.47% (6366/6400)
Train Batch: 100/352 | Loss: 0.021 | Acc: 99.55% (12742/12800)
Train Batch: 150/352 | Loss: 0.021 | Acc: 99.53% (19109/19200)
Train Batch: 200/352 | Loss: 0.020 | Acc: 99.54% (25483/25600)
Train Batch: 250/352 | Loss: 0.020 | Acc: 99.51% (31844/32000)
Train Batch: 300/352 | Loss: 0.020 | Acc: 99.51% (38213/38400)
Train Batch: 350/352 | Loss: 0.020 | Acc: 99.51% (44580/44800)
Train Batch: 352/352 | Loss: 0.020 | Acc: 99.51% (44780/45000)
Validation | Loss: 0.165 | Acc: 95.72% (4786/5000)

Training completed in 8026.46 seconds
Best validation accuracy: 0.9580 at epoch 198
Training history saved to /home/tf2387/7123pj_zzp/rd/training_history.csv
